{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114b1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b4159",
   "metadata": {},
   "source": [
    "Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dabbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04052e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5779be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce04b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "\n",
    "    if not common:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        return int(gold_toks == pred_toks), 0, 0\n",
    "    if num_same == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477cc9b",
   "metadata": {},
   "source": [
    "Metrics cho RAG + reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0094292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số câu hỏi: 21\n",
      "Số câu trả lời dự đoán: 21\n"
     ]
    }
   ],
   "source": [
    "generated_answers = 'rag_with_reranker_answers.txt'\n",
    "correct_answers = 'answers.txt'\n",
    "with open(generated_answers, 'r', encoding='utf-8') as f:\n",
    "    pred_ans = f.readlines()\n",
    "with open(correct_answers, 'r', encoding='utf-8') as f:\n",
    "    corr_ans = f.readlines()\n",
    "\n",
    "print(f\"Số câu hỏi: {len(corr_ans)}\")\n",
    "print(f\"Số câu trả lời dự đoán: {len(pred_ans)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e31cb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lines = []\n",
    "tot_f1 = tot_precision = tot_recall = exact_match = 0\n",
    "\n",
    "for i in range(len(corr_ans)):\n",
    "    reference_answers = corr_ans[i].strip().split(';')\n",
    "    best_f1 = best_precision_val = best_recall = 0\n",
    "\n",
    "    for reference_answer in reference_answers:\n",
    "        f1, prec, rec = compute_f1(reference_answer, str(pred_ans[i]))\n",
    "        if f1 >= best_f1:\n",
    "            best_f1 = f1\n",
    "            best_precision_val = prec\n",
    "            best_recall = rec\n",
    "    f1 = best_f1\n",
    "    prec = best_precision_val\n",
    "    rec = best_recall\n",
    "    ex_mtch = compute_exact(corr_ans[i], pred_ans[i])\n",
    "\n",
    "    tot_f1 += f1\n",
    "    tot_precision += prec\n",
    "    tot_recall += rec\n",
    "    exact_match += ex_mtch\n",
    "\n",
    "    output_lines.append(\n",
    "        f'Predicted answer: {pred_ans[i]}\\n'\n",
    "        f'Correct answer: {corr_ans[i]}\\n'\n",
    "        f'f1: {f1}\\nPrecision: {prec}\\nRecall: {rec}\\nExact Match: {ex_mtch}\\n'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d0df0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6129\n",
      "Precision: 0.6466\n",
      "Recall: 0.7103\n",
      "Total Exact Match: 7 / 21\n"
     ]
    }
   ],
   "source": [
    "avg_f1 = tot_f1 / len(pred_ans)\n",
    "avg_precision = tot_precision / len(pred_ans)\n",
    "avg_recall = tot_recall / len(pred_ans)\n",
    "\n",
    "print(f'F1: {avg_f1:.4f}')\n",
    "print(f'Precision: {avg_precision:.4f}')\n",
    "print(f'Recall: {avg_recall:.4f}')\n",
    "print(f'Total Exact Match: {exact_match} / {len(pred_ans)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
