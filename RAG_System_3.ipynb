{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df7953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, format_document\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "import requests\n",
    "import sys\n",
    "from langchain_core.prompts import ChatPromptTemplate, format_document\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc6bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\"  \n",
    "\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_norm = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': DEVICE},\n",
    "    encode_kwargs=encode_kwargs)\n",
    "\n",
    "db3 = Chroma(persist_directory=\"db\", embedding_function=model_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59acabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "retriever = db3.as_retriever(search_kwargs={\"k\": 100})\n",
    "def find_first_string(obj) -> Optional[str]:\n",
    "    if isinstance(obj, str):\n",
    "        return obj\n",
    "    elif isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            found = find_first_string(v)\n",
    "            if found:\n",
    "                return found\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            found = find_first_string(item)\n",
    "            if found:\n",
    "                return found\n",
    "    return None\n",
    "\n",
    "def get_answer(input_text: str, max_retries: int = 10, backoff_sec: float = 2.0) -> str:\n",
    "    url = 'https://api.groq.com/openai/v1/chat/completions'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer ' + os.environ[\"GROQ_API_KEY\"],\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"llama3-70b-8192\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 0.95\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        print(f\"Attempt {attempt} - Status: {response.status_code}\")\n",
    "\n",
    "        if response.status_code == 429:\n",
    "            # Rate limit exceeded, chờ rồi thử lại\n",
    "            retry_after = backoff_sec * attempt\n",
    "            print(f\"Rate limit reached. Retrying after {retry_after:.1f} seconds...\")\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = response.json()\n",
    "            print(\"Full JSON response:\", result)\n",
    "\n",
    "            if 'choices' in result and len(result['choices']) > 0:\n",
    "                message = result['choices'][0].get('message')\n",
    "                if message and 'content' in message:\n",
    "                    return message['content']\n",
    "\n",
    "            if 'text' in result:\n",
    "                return result['text']\n",
    "\n",
    "            if 'output' in result:\n",
    "                output = result['output']\n",
    "                return output if isinstance(output, str) else str(output)\n",
    "\n",
    "            any_text = find_first_string(result)\n",
    "            if any_text:\n",
    "                return any_text\n",
    "\n",
    "            return \"No valid response content found.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error parsing response: {e}\"\n",
    "\n",
    "    return \"Failed to get valid response after retries.\"\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from typing import Any\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def combine_documents(docs: List[Any], document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator: str = \"\\n\\n\") -> str:\n",
    "    # docs có thể là List[Document] hoặc List[Tuple]\n",
    "    doc_strings = []\n",
    "    for doc in docs:\n",
    "        # Nếu doc có attribute page_content (Document của langchain)\n",
    "        if hasattr(doc, 'page_content'):\n",
    "            doc_str = document_prompt.format(page_content=doc.page_content)\n",
    "        elif isinstance(doc, (list, tuple)) and len(doc) > 1:\n",
    "            doc_str = str(doc[1])\n",
    "        else:\n",
    "            doc_str = str(doc)\n",
    "        doc_strings.append(doc_str)\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "def split_questions(text: str, delimiter: str = '?') -> List[str]:\n",
    "    if delimiter not in text:\n",
    "        return [text.strip()]  # Trả về nguyên văn nếu không có dấu câu hỏi\n",
    "    questions = text.split(delimiter)\n",
    "    cleaned_questions = []\n",
    "    for q in questions:\n",
    "        q = q.strip()\n",
    "        if not q:\n",
    "            continue\n",
    "        # Lấy phần sau dấu chấm đầu tiên nếu có, rồi thêm lại dấu hỏi\n",
    "        if \". \" in q:\n",
    "            q = q.split(\". \", 1)[-1]\n",
    "        cleaned_questions.append(q + delimiter)\n",
    "    return cleaned_questions\n",
    "def combine_documents_2(docs, document_separator=\"\\n\\n\"):\n",
    "    combined_docs = []\n",
    "    for doc in docs:\n",
    "        combined_docs.append(doc[1])\n",
    "    return document_separator.join(combined_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badaed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_19228\\3189216533.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  all_docs.extend(retriever.get_relevant_documents(question))\n",
      "c:\\Users\\LOQ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "[]\n",
      "\"What bank, which is the 5th largest in the US, is based in Pittsburgh?\"\n",
      "\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "[]\n",
      "How many bridges does Pittsburgh have?\n",
      "\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "[]\n",
      "Who named the city of Pittsburgh?\n",
      "\n",
      "Attempt 1 - Status: 401\n",
      "Full JSON response: {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "def _unique_documents(documents):\n",
    "    return [doc for i, doc in enumerate(documents) if doc not in documents[:i]]\n",
    "delimiter = '?'\n",
    "\n",
    "with open('Data_test/test_questions.csv', 'r') as file:\n",
    "    questions = file.readlines()\n",
    "    \n",
    "for question in questions:\n",
    "    \n",
    "    input_text_for_ques = f\"\"\"\n",
    "    [TASK]: Write the below question in 3 different ways.\n",
    "    [QUESTION]: {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    diff_questions = get_answer(input_text_for_ques)\n",
    "    \n",
    "    paraphrased_ques = split_questions(diff_questions, delimiter)\n",
    "    paraphrased_ques = paraphrased_ques[:-1]\n",
    "    print(paraphrased_ques)\n",
    "    \n",
    "    all_docs = []\n",
    "    for single_question in paraphrased_ques:\n",
    "        all_docs.extend(retriever.get_relevant_documents(single_question))\n",
    "        \n",
    "    all_docs.extend(retriever.get_relevant_documents(question))\n",
    "    \n",
    "    unique_docs = _unique_documents(all_docs)\n",
    "\n",
    "    prompt_start = \"\"\"You are an assistant for question-answering tasks and the questions are related to the University of Pittsburgh and Carnegie Mellon University (CMU). Do not exceed one sentence for the answer. Do not be verbose when generating the answer. Give out the answer directly even if it does not form a coherent sentence.\"\"\"\n",
    "\n",
    "    docs_to_rerank = []\n",
    "    for i in range(len(unique_docs)):\n",
    "        docs_to_rerank.append([question, str(unique_docs[i])])\n",
    "    scores = reranker.compute_score(docs_to_rerank)\n",
    "\n",
    "    combined_data = list(zip(docs_to_rerank, scores))\n",
    "    sorted_data = sorted(combined_data, key=lambda x: x[1], reverse=True)\n",
    "    sorted_docs_to_rerank, sorted_scores = zip(*sorted_data)\n",
    "    top_k_docs = sorted_docs_to_rerank[:10]\n",
    "    context = combine_documents_2(top_k_docs)\n",
    "\n",
    "    input_text = prompt_start + \"Question: \" + question + \"Context: \" + context + \"Answer: \"\n",
    "    print(question)\n",
    "    time.sleep(2)\n",
    "\n",
    "    answer = get_answer(input_text)\n",
    "    with open('system_output_3.txt', 'a', encoding = 'utf-8') as output_file:\n",
    "        output_file.write(f'{answer}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
