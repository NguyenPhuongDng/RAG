{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50814017",
   "metadata": {},
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517aeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_wikipedia(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Lá»—i khi truy cáº­p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'id': 'bodyContent'})\n",
    "\n",
    "    if not content_div:\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y ná»™i dung chÃ­nh.\")\n",
    "        return\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(allowed_tags):\n",
    "            # XoÃ¡ chÃº thÃ­ch trong tháº» <p>\n",
    "            if tag.name == 'p':\n",
    "                for sup in tag.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u ná»™i dung tá»« {url} vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "# crawl_wikipedia(\"https://en.wikipedia.org/wiki/Pittsburgh\", \"pittsburgh.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dda932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://en.wikipedia.org/wiki/Pittsburgh vÃ o 'pittsburgh.txt'\n",
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://en.wikipedia.org/wiki/History_of_Pittsburgh vÃ o 'history_of_pittsburgh.txt'\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://en.wikipedia.org/wiki/Pittsburgh\"\n",
    "url2 = \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\"\n",
    "output_file1 = \"pittsburgh.txt\"\n",
    "output_file2 = \"history_of_pittsburgh.txt\"\n",
    "\n",
    "crawl_wikipedia(url1, output_file1)\n",
    "crawl_wikipedia(url2, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780bdc",
   "metadata": {},
   "source": [
    "britannica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_britannica(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Lá»—i khi truy cáº­p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # CÃ¡c tháº» muá»‘n giá»¯ láº¡i theo thá»© tá»± xuáº¥t hiá»‡n\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u ná»™i dung tá»« {url} vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "# crawl_britannica(\"https://www.britannica.com/place/Pittsburgh\", \"pittsburgh_britannica.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718a27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://www.britannica.com/place/Pittsburgh vÃ o 'pittsburgh_britannica.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.britannica.com/place/Pittsburgh\"\n",
    "output_file = \"pittsburgh_britannica.txt\"\n",
    "crawl_britannica(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c3b5d",
   "metadata": {},
   "source": [
    "dynamic web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def crawl_dynamic_site(url, output_file):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # cháº¡y áº©n\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    service = Service(r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # chá» trang load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u ná»™i dung tá»« {url} vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "# crawl_dynamic_site(\"https://www.visitpittsburgh.com/\", \"visitpittsburgh.txt\", driver_path=\"/path/to/chromedriver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4d6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://www.visitpittsburgh.com/ vÃ o 'Pittsburgh webpage.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.visitpittsburgh.com/\"\n",
    "output_file = \"Pittsburgh webpage.txt\"\n",
    "crawl_dynamic_site(url=url, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "757f6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9622_amusement_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9626_payroll_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9623_isp_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9624_local_services_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9625_parking_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9627_uf_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: change-in-business-status-form-04.2025.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6492_2636_10_taxpayers_bill_of_rights_4-26-2018.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 16958_2022_tax_rate_by_tax_type.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 16957_2022_tax_due_date_calendar_.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8271_facility_usage_fee_information_for_performers_and_contracting_parties.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: firesale.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8398_payroll_expense_tax__et__allocation_schedule_form.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6825_payroll_expense_tax_allocation_schedule_for_professional_organization_form_instructions8.15.19.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8397_local_services_tax_ls-1_allocation_schedule_form.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6822_local_service_tax_allocation_schedule_for_professional_employer_organization_form_instructions8.15.19.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8403_general_contractor_detail_report.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 7065_general_contractor_detail_report_instructions.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: ls-tax-exemption-certificate-2025.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6819__lst_refund_form.pdf\n",
      "\n",
      "ğŸ“„ ÄÃ£ ghi toÃ n bá»™ ná»™i dung vÃ o 'pittsburgh_tax.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def crawl_and_extract(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"âŒ Lá»—i truy cáº­p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'class': 'main-container clearfix'})\n",
    "    if not content_div:\n",
    "        print(\"âŒ KhÃ´ng tÃ¬m tháº¥y vÃ¹ng ná»™i dung chÃ­nh.\")\n",
    "        return\n",
    "\n",
    "    tags = ['h1', 'h2', 'h3', 'h4', 'p', 'a']\n",
    "    found_regulation = False\n",
    "    base_path = \"pdf_temp\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(tags):\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                text = tag.get_text(strip=True)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if tag.name == 'h1':\n",
    "                    f.write(f\"# {text}\\n\\n\")\n",
    "                elif tag.name == 'h2':\n",
    "                    f.write(f\"## {text}\\n\\n\")\n",
    "                elif tag.name == 'h3':\n",
    "                    f.write(f\"### {text}\\n\\n\")\n",
    "                elif tag.name == 'h4':\n",
    "                    f.write(f\"#### {text}\\n\\n\")\n",
    "                    if 'regulations' in text.lower():\n",
    "                        found_regulation = True\n",
    "                    else:\n",
    "                        found_regulation = False  # reset khi gáº·p h4 khÃ¡c\n",
    "\n",
    "            elif tag.name == 'p':\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    f.write(text + '\\n\\n')\n",
    "\n",
    "            elif tag.name == 'a' and found_regulation:\n",
    "                href = tag.get('href', '')\n",
    "                if href.endswith('.pdf'):\n",
    "                    pdf_url = urljoin(url, href)\n",
    "                    filename = os.path.basename(href)\n",
    "                    local_pdf = os.path.join(base_path, filename)\n",
    "\n",
    "                    try:\n",
    "                        pdf_res = requests.get(pdf_url, headers=headers)\n",
    "                        with open(local_pdf, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_res.content)\n",
    "                        f.write(f\"ğŸ“ Ná»™i dung tá»« file PDF: {filename}\\n\\n\")\n",
    "\n",
    "                        # TrÃ­ch ná»™i dung PDF\n",
    "                        doc = fitz.open(local_pdf)\n",
    "                        for page in doc:\n",
    "                            text = page.get_text().strip()\n",
    "                            if text:\n",
    "                                f.write(text + '\\n\\n')\n",
    "                        doc.close()\n",
    "                        print(f\"âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: {filename}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"[Lá»—i khi táº£i hoáº·c Ä‘á»c PDF {filename}: {e}]\\n\\n\")\n",
    "\n",
    "    print(f\"\\nğŸ“„ ÄÃ£ ghi toÃ n bá»™ ná»™i dung vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "crawl_and_extract(\n",
    "    \"https://www.pittsburghpa.gov/City-Government/Finances-Budget/Taxes/Tax-Forms\",\n",
    "    \"pittsburgh_tax.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e509a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ trÃ­ch xuáº¥t vÄƒn báº£n vÃ o '2024 Operating Budget.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # áº¨n cáº£nh bÃ¡o SSL\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url: str, output_txt: str = \"output.txt\"):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, verify=False)  # âš ï¸ Bá» qua SSL verify\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        doc = fitz.open(\"temp.pdf\")\n",
    "        all_text = []\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            # Giá»¯ nguyÃªn xuá»‘ng dÃ²ng, nhÆ°ng loáº¡i khoáº£ng tráº¯ng thá»«a\n",
    "            cleaned_lines = [line.strip() for line in page_text.splitlines() if line.strip()]\n",
    "            all_text.extend(cleaned_lines)\n",
    "            all_text.append(\"\")  # ThÃªm dÃ²ng trá»‘ng giá»¯a cÃ¡c trang (tÃ¹y chá»n)\n",
    "\n",
    "        # Ghi ra file, má»—i dÃ²ng lÃ  1 dÃ²ng vÄƒn báº£n rÃµ rÃ ng\n",
    "        with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_text))\n",
    "\n",
    "        print(f\"âœ… ÄÃ£ trÃ­ch xuáº¥t vÄƒn báº£n vÃ o '{output_txt}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i: {e}\")\n",
    "\n",
    "extract_text_from_pdf_url(\n",
    "     \"https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf\",\n",
    "     \"2024 Operating Budget.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb5fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: https://www.cmu.edu/about/\n",
      "âœ… Crawled: https://www.cmu.edu/academics/interdisciplinary-programs.html\n",
      "âœ… Crawled: https://www.library.cmu.edu/\n",
      "âœ… Crawled: https://www.cmu.edu/academics/learning-for-a-lifetime.html\n",
      "âœ… Crawled: https://www.cmu.edu/admission/student-community-blog\n",
      "âœ… Crawled: https://www.cmu.edu/graduate/prospective/index.html\n",
      "âœ… Crawled: https://www.cmu.edu/leadership/\n",
      "âœ… Crawled: https://www.cmu.edu/about/mission.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/history.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/traditions.html\n",
      "âœ… Crawled: https://www.cmu.edu/inclusive-excellence/\n",
      "âœ… Crawled: https://www.cmu.edu/about/pittsburgh.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/rankings.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/awards.html\n",
      "âœ… Crawled: https://www.cmu.edu/visit//visitor-information\n",
      "âœ… Crawled: https://www.cmu.edu/research/centers-and-institutes.html\n",
      "âœ… Crawled: https://www.cmu.edu/student-experience/index.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "urls = [\n",
    "    \"https://www.cmu.edu/about/\",\n",
    "    \"https://www.cmu.edu/academics/interdisciplinary-programs.html\",\n",
    "    \"https://www.library.cmu.edu/\",\n",
    "    \"https://www.cmu.edu/academics/learning-for-a-lifetime.html\",\n",
    "    \"https://www.cmu.edu/admission/student-community-blog\",\n",
    "    \"https://www.cmu.edu/graduate/prospective/index.html\",\n",
    "    \"https://www.cmu.edu/leadership/\",\n",
    "    \"https://www.cmu.edu/about/mission.html\",\n",
    "    \"https://www.cmu.edu/about/history.html\",\n",
    "    \"https://www.cmu.edu/about/traditions.html\",\n",
    "    \"https://www.cmu.edu/inclusive-excellence/\",\n",
    "    \"https://www.cmu.edu/about/pittsburgh.html\",\n",
    "    \"https://www.cmu.edu/about/rankings.html\",\n",
    "    \"https://www.cmu.edu/about/awards.html\",\n",
    "    \"https://www.cmu.edu/visit//visitor-information\",\n",
    "    \"https://www.cmu.edu/research/centers-and-institutes.html\",\n",
    "    \"https://www.cmu.edu/student-experience/index.html\",\n",
    "]\n",
    "\n",
    "def extract_header(url):\n",
    "    # Láº¥y pháº§n sau cÃ¹ng cá»§a URL Ä‘á»ƒ lÃ m tiÃªu Ä‘á»\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip(\"/\").split(\"/\")[-1]\n",
    "    if not path or path == \"index.html\":\n",
    "        path = parsed.path.strip(\"/\").split(\"/\")[-2] if len(parsed.path.strip(\"/\").split(\"/\")) > 1 else \"home\"\n",
    "    path = re.sub(r'\\.html$', '', path)\n",
    "    return \"#\" + path.lower().replace(\" \", \"-\")\n",
    "\n",
    "def clean_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    main = soup.find(\"main\") or soup.body\n",
    "\n",
    "    if main is None:\n",
    "        return \"\"\n",
    "\n",
    "    for tag in main.find_all([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = main.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def crawl_and_write(urls, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                print(f\"âœ… Crawled: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Lá»—i truy cáº­p {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            header = extract_header(url)\n",
    "            text = clean_text(response.text)\n",
    "\n",
    "            if text:\n",
    "                f.write(header + \"\\n\")\n",
    "                f.write(text + \"\\n\\n\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y ná»™i dung á»Ÿ {url}\")\n",
    "\n",
    "# Gá»i hÃ m crawl\n",
    "crawl_and_write(urls, \"cmu_about.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027b9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang crawl trang 1...\n",
      "ğŸ” Äang crawl trang 2...\n",
      "ğŸ” Äang crawl trang 3...\n",
      "ğŸ” Äang crawl trang 4...\n",
      "ğŸ” Äang crawl trang 5...\n",
      "ğŸ” Äang crawl trang 6...\n",
      "ğŸ” Äang crawl trang 7...\n",
      "ğŸ” Äang crawl trang 8...\n",
      "ğŸ” Äang crawl trang 9...\n",
      "ğŸ” Äang crawl trang 10...\n",
      "ğŸ” Äang crawl trang 11...\n",
      "ğŸ” Äang crawl trang 12...\n",
      "ğŸ” Äang crawl trang 13...\n",
      "ğŸ” Äang crawl trang 14...\n",
      "ğŸ” Äang crawl trang 15...\n",
      "ğŸ” Äang crawl trang 16...\n",
      "ğŸ” Äang crawl trang 17...\n",
      "ğŸ” Äang crawl trang 18...\n",
      "ğŸ” Äang crawl trang 19...\n",
      "ğŸ” Äang crawl trang 20...\n",
      "ğŸ” Äang crawl trang 21...\n",
      "ğŸ” Äang crawl trang 22...\n",
      "ğŸ” Äang crawl trang 23...\n",
      "ğŸ” Äang crawl trang 24...\n",
      "ğŸ” Äang crawl trang 25...\n",
      "ğŸ” Äang crawl trang 26...\n",
      "ğŸ” Äang crawl trang 27...\n",
      "ğŸ” Äang crawl trang 28...\n",
      "ğŸ” Äang crawl trang 29...\n",
      "ğŸ” Äang crawl trang 30...\n",
      "ğŸ” Äang crawl trang 31...\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o 'Downtown Pittsburgh events calendar.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_all_events(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 32):  # tá»« 1 Ä‘áº¿n 31\n",
    "            url = f\"https://downtownpittsburgh.com/events/?n=5&d={page}&y=2025\"\n",
    "            print(f\"ğŸ” Äang crawl trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ Lá»—i truy cáº­p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            h1_tags = soup.find_all('h1')\n",
    "\n",
    "            if not h1_tags:\n",
    "                print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y sá»± kiá»‡n trÃªn trang {page}\")\n",
    "                continue\n",
    "\n",
    "            for h1 in h1_tags:\n",
    "                link_tag = h1.find('a')\n",
    "                if not link_tag:\n",
    "                    continue\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                href = link_tag.get('href', '')\n",
    "                full_url = 'https://downtownpittsburgh.com' + href\n",
    "\n",
    "                sibling = h1.next_sibling\n",
    "                date_text = None\n",
    "                description_parts = []\n",
    "\n",
    "                while sibling:\n",
    "                    if not hasattr(sibling, 'get_text'):\n",
    "                        sibling = sibling.next_sibling\n",
    "                        continue\n",
    "\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text:\n",
    "                        if not date_text and ('am' in text.lower() or 'pm' in text.lower() or '-' in text):\n",
    "                            date_text = text\n",
    "                        elif text == \"READ MORE\":\n",
    "                            break\n",
    "                        else:\n",
    "                            description_parts.append(text)\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "                if not date_text:\n",
    "                    date_text = \"No date\"\n",
    "                description = \" \".join(description_parts).strip() if description_parts else \"No description\"\n",
    "\n",
    "                f.write(f\"## [{title}]({full_url})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date_text}\\n\")\n",
    "                f.write(f\"**Description**: {description}\\n\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o '{output_file}'\")\n",
    "\n",
    "\n",
    "crawl_all_events(\"Downtown Pittsburgh events calendar.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "477a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang xá»­ lÃ½ trang 1...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 2...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 3...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 4...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 5...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 6...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 7...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 8...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 9...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 10...\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u sá»± kiá»‡n vÃ o 'citypaper_events.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_citypaper_detailed(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 11):\n",
    "            url = f\"https://www.pghcitypaper.com/pittsburgh/EventSearch?page={page}&v=d\"\n",
    "            print(f\"ğŸ” Äang xá»­ lÃ½ trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ Lá»—i truy cáº­p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            events = soup.find_all('p', class_='fdn-teaser-headline')\n",
    "\n",
    "            for headline in events:\n",
    "                # Láº¥y tiÃªu Ä‘á» vÃ  link\n",
    "                a_tag = headline.find('a', href=True)\n",
    "                if not a_tag:\n",
    "                    continue\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                link = a_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://www.pghcitypaper.com' + link\n",
    "\n",
    "                # TÃ¬m cÃ¡c pháº§n cÃ²n láº¡i thÃ´ng qua cha cá»§a headline\n",
    "                parent = headline.find_parent()\n",
    "                date_tag = parent.find_next('p', class_='fdn-teaser-subheadline')\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"No date\"\n",
    "\n",
    "                loc_tag = parent.find_next('a', class_='fdn-event-teaser-location-link')\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else \"No location\"\n",
    "\n",
    "                addr_tag = parent.find_next('p', class_='fdn-inline-split-list')\n",
    "                address = addr_tag.get_text(strip=True) if addr_tag else \"No address\"\n",
    "\n",
    "                desc_tag = parent.find_next('div', class_='fdn-teaser-description')\n",
    "                desc = desc_tag.get_text(strip=True) if desc_tag else \"No description\"\n",
    "\n",
    "                # Ghi ra file\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date}\\n\")\n",
    "                f.write(f\"**Location**: {location}, {address}\\n\")\n",
    "                f.write(f\"**Description**: {desc}\\n\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u sá»± kiá»‡n vÃ o '{output_file}'\")\n",
    "\n",
    "# Gá»i hÃ m:\n",
    "crawl_citypaper_detailed(\"citypaper_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6386f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250401\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250402\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250403\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250404\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250405\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250406\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250407\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250408\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250409\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250410\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250411\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250412\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250413\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250414\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250415\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250416\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250417\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250418\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250419\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250420\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250421\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250422\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250423\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250424\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250425\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250426\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250427\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250428\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250429\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250430\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250501\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250502\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250503\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250504\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250505\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250506\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250507\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250508\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250509\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250510\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250511\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250512\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250513\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o 'cmu_events.txt'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def crawl_cmu_events_selenium(start_date, end_date, output_file):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # khÃ´ng má»Ÿ cá»­a sá»• trÃ¬nh duyá»‡t\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime('%Y%m%d')\n",
    "            url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "            print(f\"ğŸ” Äang xá»­ lÃ½: {url}\")\n",
    "\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # Ä‘á»£i trang load JS xong\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            events = soup.find_all('div', class_='lw_cal_event_info')\n",
    "\n",
    "            if not events:\n",
    "                current_date += timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            for event in events:\n",
    "                title_tag = event.find('div', class_='lw_events_title')\n",
    "                title = title_tag.a.get_text(strip=True) if title_tag and title_tag.a else \"KhÃ´ng cÃ³ tiÃªu Ä‘á»\"\n",
    "                link = title_tag.a['href'] if title_tag and title_tag.a else \"#\"\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://events.cmu.edu' + link\n",
    "\n",
    "                location = event.find('div', class_='lw_events_location')\n",
    "                time_tag = event.find('div', class_='lw_events_time')\n",
    "                summary = event.find('div', class_='lw_events_summary')\n",
    "\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date**: {current_date.strftime('%Y-%m-%d')}\\n\")\n",
    "                f.write(f\"**Time**: {time_tag.get_text(strip=True) if time_tag else 'KhÃ´ng rÃµ'}\\n\")\n",
    "                f.write(f\"**Location**: {location.get_text(strip=True) if location else 'KhÃ´ng rÃµ'}\\n\")\n",
    "                f.write(f\"**Summary**: {summary.get_text(strip=True) if summary else 'KhÃ´ng cÃ³'}\\n\\n\")\n",
    "\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o '{output_file}'\")\n",
    "\n",
    "# Gá»i hÃ m\n",
    "start = datetime(2025, 4, 1)\n",
    "end = datetime(2025, 5, 13)\n",
    "crawl_cmu_events_selenium(start, end, \"cmu_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5f43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KhÃ´ng tÃ¬m tháº¥y sá»± kiá»‡n nÃ o. Vui lÃ²ng kiá»ƒm tra file crawl_log.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Cáº¥u hÃ¬nh logging\n",
    "logging.basicConfig(filename='crawl_log.txt', level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  khoáº£ng tráº¯ng thá»«a\n",
    "    return re.sub(r'\\s+', ' ', text.strip()) if text else 'N/A'\n",
    "\n",
    "def crawl_events():\n",
    "    url = \"https://www.visitpittsburgh.com/events-festivals/?page=11\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Äang gá»­i yÃªu cáº§u tá»›i {url}\")\n",
    "        # Gá»­i yÃªu cáº§u GET\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        logging.info(\"YÃªu cáº§u thÃ nh cÃ´ng\")\n",
    "        \n",
    "        # PhÃ¢n tÃ­ch HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        logging.info(\"ÄÃ£ phÃ¢n tÃ­ch HTML\")\n",
    "        \n",
    "        # TÃ¬m táº¥t cáº£ cÃ¡c sá»± kiá»‡n\n",
    "        events = soup.find_all('div', class_='card card--common card--listing')\n",
    "        logging.info(f\"TÃ¬m tháº¥y {len(events)} sá»± kiá»‡n\")\n",
    "        \n",
    "        if not events:\n",
    "            logging.warning(\"KhÃ´ng tÃ¬m tháº¥y sá»± kiá»‡n nÃ o. Kiá»ƒm tra cáº¥u trÃºc HTML.\")\n",
    "            print(\"KhÃ´ng tÃ¬m tháº¥y sá»± kiá»‡n nÃ o. Vui lÃ²ng kiá»ƒm tra file crawl_log.txt\")\n",
    "            return\n",
    "        \n",
    "        # Má»Ÿ file Ä‘á»ƒ lÆ°u dá»¯ liá»‡u\n",
    "        with open('pittsburgh_events.txt', 'w', encoding='utf-8') as f:\n",
    "            for i, event in enumerate(events, 1):\n",
    "                try:\n",
    "                    # Láº¥y tiÃªu Ä‘á»\n",
    "                    title_elem = event.find('a', class_='card_heading')\n",
    "                    title = clean_text(title_elem.text)\n",
    "                    \n",
    "                    # Láº¥y ngÃ y\n",
    "                    date_elem = event.find('p', class_='date-heading card_date-heading')\n",
    "                    date = clean_text(date_elem.text)\n",
    "                    \n",
    "                    # Láº¥y Ä‘á»‹a chá»‰\n",
    "                    address_elem = event.find('div', class_='card____address')\n",
    "                    address = clean_text(address_elem.text)\n",
    "                    \n",
    "                    # Láº¥y sá»‘ Ä‘iá»‡n thoáº¡i\n",
    "                    phone_elem = event.find('div', class_='card____phone')\n",
    "                    phone = clean_text(phone_elem.text)\n",
    "                    \n",
    "                    # Ghi vÃ o file\n",
    "                    f.write(f\"Title: {title}\\n\")\n",
    "                    f.write(f\"Date: {date}\\n\")\n",
    "                    f.write(f\"Address: {address}\\n\")\n",
    "                    f.write(f\"Phone: {phone}\\n\")\n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                    \n",
    "                    logging.info(f\"ÄÃ£ crawl sá»± kiá»‡n {i}: {title}\")\n",
    "                    \n",
    "                except AttributeError as e:\n",
    "                    logging.error(f\"Lá»—i khi crawl sá»± kiá»‡n {i}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        print(\"Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o pittsburgh_events.txt\")\n",
    "        logging.info(\"HoÃ n táº¥t crawl dá»¯ liá»‡u\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Lá»—i khi truy cáº­p trang web: {e}\")\n",
    "        logging.error(f\"Lá»—i khi truy cáº­p trang web: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i khÃ¡c: {e}\")\n",
    "        logging.error(f\"Lá»—i khÃ¡c: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0a50244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lá»—i khi truy cáº­p https://carnegiemuseums.org/: 403 Client Error: Forbidden for url: https://carnegiemuseums.org/\n",
      "âœ… ÄÃ£ crawl vÃ  ghi dá»¯ liá»‡u vÃ o cmu_events.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def extract_event_info(soup):\n",
    "    title = soup.find('h1')\n",
    "    title_text = title.get_text(strip=True) if title else \"UnKnown Title\"\n",
    "\n",
    "    # TÃ¬m ngÃ y náº¿u cÃ³ trong Ä‘oáº¡n vÄƒn hoáº·c tháº» time\n",
    "    time_text = \"\"\n",
    "    time_tag = soup.find('time')\n",
    "    if time_tag:\n",
    "        time_text = time_tag.get_text(strip=True)\n",
    "    else:\n",
    "        # TÃ¬m chuá»—i giá»‘ng ngÃ y thÃ¡ng\n",
    "        text_block = soup.get_text()\n",
    "        match = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(,)?\\s+\\d{4}', text_block)\n",
    "        if match:\n",
    "            time_text = match.group(0)\n",
    "\n",
    "    # TÃ¬m mÃ´ táº£\n",
    "    desc_div = soup.find('div', class_='content')\n",
    "    description = desc_div.get_text(separator='\\n', strip=True) if desc_div else \"\"\n",
    "\n",
    "    # Náº¿u khÃ´ng cÃ³ mÃ´ táº£, láº¥y Ä‘oáº¡n <p> Ä‘áº§u tiÃªn\n",
    "    if not description:\n",
    "        p_tag = soup.find('p')\n",
    "        if p_tag:\n",
    "            description = p_tag.get_text(strip=True)\n",
    "\n",
    "    return title_text, time_text, description\n",
    "\n",
    "def crawl(url, depth=0, max_depth=2, file=None):\n",
    "    if url in visited or depth > max_depth:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i khi truy cáº­p {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title, time_text, description = extract_event_info(soup)\n",
    "    if title or description:\n",
    "        file.write(f\"URL: {url}\\n\")\n",
    "        file.write(f\"Title: {title}\\n\")\n",
    "        if time_text:\n",
    "            file.write(f\"Time: {time_text}\\n\")\n",
    "        file.write(f\"Describe:\\n{description}\\n\")\n",
    "        file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "    # Äá»‡ quy vÃ o cÃ¡c link ná»™i bá»™ liÃªn quan\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        full_url = urljoin(url, href)\n",
    "        parsed = urlparse(full_url)\n",
    "\n",
    "        # Chá»‰ crawl cÃ¡c trang cá»§a cmu.edu vÃ  chá»©a tá»« khÃ³a liÃªn quan\n",
    "        if ('cmu.edu' in parsed.netloc) and ('events' in parsed.path):\n",
    "            crawl(full_url, depth + 1, max_depth, file)\n",
    "\n",
    "# Gá»i crawl vÃ  ghi dá»¯ liá»‡u\n",
    "with open(\"campus events page.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    start_url = \"https://carnegiemuseums.org/\"\n",
    "    crawl(start_url, max_depth=20, file=f)\n",
    "\n",
    "print(\"âœ… ÄÃ£ crawl vÃ  ghi dá»¯ liá»‡u vÃ o cmu_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9b7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://classicalvoiceamerica.org/2025/04/29/motherhoods-painful-art-splashed-on-intimate-canvas-of-chamber/: 403 Client Error: Forbidden for url: https://classicalvoiceamerica.org/2025/04/29/motherhoods-painful-art-splashed-on-intimate-canvas-of-chamber/\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://pittsburgh.tablemagazine.com/blog/woman-with-eyes-closed/: 403 Client Error: Forbidden for url: https://pittsburgh.tablemagazine.com/blog/woman-with-eyes-closed/\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/theater-dance/2025/01/27/pittsburgh-opera-armida-review-tickets-resident-artists/stories/202501270050: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/theater-dance/2025/01/27/pittsburgh-opera-armida-review-tickets-resident-artists/stories/202501270050 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8BEF6530>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2025/01/22/music-101-mozart-vs-bach/stories/202501170056: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2025/01/22/music-101-mozart-vs-bach/stories/202501170056 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8BEF4490>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/11/11/pittsburgh-opera-review-cavalleria-rusticana-pagliacci-tickets-chorus/stories/202411110068: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/11/11/pittsburgh-opera-review-cavalleria-rusticana-pagliacci-tickets-chorus/stories/202411110068 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B18C8B0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/10/31/pittsburgh-opera-chorus-audition-rehearsal-choir-singer/stories/202410300080: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/10/31/pittsburgh-opera-chorus-audition-rehearsal-choir-singer/stories/202410300080 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B4F8790>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/10/08/pittsburgh-opera-tosca-review-puccini/stories/202410070079: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/10/08/pittsburgh-opera-tosca-review-puccini/stories/202410070079 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8D579270>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/10/02/pittsburgh-opera-tosca-tickets-music-101/stories/202409260106?utm_campaign=Newsletter%20Editorial%20Calendar%202024-25%20Season&utm_medium=email&_hsenc=p2ANqtz-8O7I6kgxBIX8eLhOCjCcgQaEJbBe9bluEnPivxnIueXI2ZoH9Hb5Q0UcFDYbud02GmCXnp_m4ST1LWh_YhRU596_Tb-dnMB2ByxYUfMN2nPhokmOA&_hsmi=2&utm_content=2&utm_source=hs_email: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/10/02/pittsburgh-opera-tosca-tickets-music-101/stories/202409260106?utm_campaign=Newsletter%20Editorial%20Calendar%202024-25%20Season&utm_medium=email&_hsenc=p2ANqtz-8O7I6kgxBIX8eLhOCjCcgQaEJbBe9bluEnPivxnIueXI2ZoH9Hb5Q0UcFDYbud02GmCXnp_m4ST1LWh_YhRU596_Tb-dnMB2ByxYUfMN2nPhokmOA&_hsmi=2&utm_content=2&utm_source=hs_email (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B73AE60>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/07/25/pittsburgh-opera-conductor-antony-walker-contract/stories/202407230067: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/07/25/pittsburgh-opera-conductor-antony-walker-contract/stories/202407230067 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8D0EC1C0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/05/02/pittsburgh-opera-passion-mary-cardwell-dawson-national-negro-company/stories/202405010083: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/05/02/pittsburgh-opera-passion-mary-cardwell-dawson-national-negro-company/stories/202405010083 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8DBDD750>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://triblive.com/aande/theater-arts/pittsburgh-operas-passion-of-mary-cardwell-dawson-is-a-performance-to-remember/: 403 Client Error: Forbidden for url: https://triblive.com/aande/theater-arts/pittsburgh-operas-passion-of-mary-cardwell-dawson-is-a-performance-to-remember/\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://operawire.com/five-winners-of-metropolitan-operas-2024-laffont-competition-announced/: 403 Client Error: Forbidden for url: https://operawire.com/five-winners-of-metropolitan-operas-2024-laffont-competition-announced/\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2024/01/16/pittsburgh-opera-iphigenie-tickets-baroque-inflation-set-cost-steven-grair/stories/202401100101: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/01/16/pittsburgh-opera-iphigenie-tickets-baroque-inflation-set-cost-steven-grair/stories/202401100101 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B4FB5E0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/2023/11/07/pittsburgh-opera-flying-dutchman-tickets-wagner-sam-helfrich-ghost-ship/stories/202311030067: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/2023/11/07/pittsburgh-opera-flying-dutchman-tickets-wagner-sam-helfrich-ghost-ship/stories/202311030067 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8DCDA080>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link https://www.post-gazette.com/ae/music/2023/10/15/pittsburgh-opera-review-barber-of-seville-tickets-figaro-season/stories/202310150123: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2023/10/15/pittsburgh-opera-review-barber-of-seville-tickets-figaro-season/stories/202310150123 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B3173D0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "âœ… ÄÃ£ lÆ°u toÃ n bá»™ tin tá»©c vÃ  ná»™i dung bÃ i viáº¿t vÃ o 'pittsburgh_opera_news.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "MAIN_URL = \"https://pittsburghopera.org/news-and-announcements\"\n",
    "\n",
    "def get_article_content(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # Láº¥y toÃ n bá»™ vÄƒn báº£n tá»« bÃ i viáº¿t\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "        #return content[:2000] + \"...\" if len(content) > 2000 else content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ KhÃ´ng thá»ƒ truy cáº­p link {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_pittsburgh_opera():\n",
    "    try:\n",
    "        res = requests.get(MAIN_URL)\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i khi truy cáº­p: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    items = soup.select(\"div.home-news-item\")\n",
    "\n",
    "    with open(\"pittsburgh_opera_news.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in items:\n",
    "            title_tag = item.select_one(\".title span\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"KhÃ´ng cÃ³ tiÃªu Ä‘á»\"\n",
    "\n",
    "            link_tag = item.select_one(\"a\")\n",
    "            link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"\"\n",
    "\n",
    "            f.write(f\"ğŸ“° {title}\\n\")\n",
    "            f.write(f\"ğŸ”— {link}\\n\")\n",
    "\n",
    "            if link.startswith(\"http\"):\n",
    "                article = get_article_content(link)\n",
    "                if article:\n",
    "                    f.write(f\"ğŸ“„ Ná»™i dung bÃ i viáº¿t:\\n{article}\\n\")\n",
    "\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    print(\"âœ… ÄÃ£ lÆ°u toÃ n bá»™ tin tá»©c vÃ  ná»™i dung bÃ i viáº¿t vÃ o 'pittsburgh_opera_news.txt'\")\n",
    "\n",
    "crawl_pittsburgh_opera()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10fb9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/joinrenew/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/timed-tickets/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/plan-a-visit\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/things-to-do/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/things-to-do/view-our-exhibitions/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/things-to-do/explore-our-collections/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/things-to-do/learn-with-us/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/events/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/plan-a-visit/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/members-only-events/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/benefits/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/faq/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/reciprocal-privileges/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/donate/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/teen-membership/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/membership/access-for-all/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/carnegieconnectors/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/join-support/donate/corporate-gifts/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/great-event-spaces/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/carnegie-museums-in-oakland/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/carnegie-science-center/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/the-andy-warhol-museum/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/weddings-at-the-museums/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/contact-us/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/carnegie-magazine/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/tag/art/\n",
      "ğŸ•¸ï¸ Äang crawl: https://carnegiemuseums.org/tag/science-nature/\n",
      "âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u vÃ o 'carnegie_museums_data.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://carnegiemuseums.org/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def get_soup(url):\n",
    "    res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    res.raise_for_status()\n",
    "    return BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "def is_valid_link(href):\n",
    "    if not href or href.startswith(\"#\"):\n",
    "        return False\n",
    "    parsed = urlparse(href)\n",
    "    return not parsed.netloc or parsed.netloc == urlparse(BASE_URL).netloc\n",
    "\n",
    "def crawl_site(start_url, max_pages=20):\n",
    "    to_visit = [start_url]\n",
    "    result = []\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"ğŸ•¸ï¸ Äang crawl: {url}\")\n",
    "            soup = get_soup(url)\n",
    "            visited.add(url)\n",
    "\n",
    "            title = soup.title.string.strip() if soup.title else \"KhÃ´ng cÃ³ tiÃªu Ä‘á»\"\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            result.append((url, title, content[:1000] + \"...\" if len(content) > 1000 else content))\n",
    "\n",
    "            # Láº¥y thÃªm link Ä‘á»ƒ crawl tiáº¿p\n",
    "            links = soup.find_all(\"a\", href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if is_valid_link(href):\n",
    "                    full_url = urljoin(BASE_URL, href)\n",
    "                    if full_url not in visited and full_url not in to_visit:\n",
    "                        to_visit.append(full_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Lá»—i vá»›i {url}: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_to_file(data, filename=\"carnegie_museums_data.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url, title, content in data:\n",
    "            f.write(f\"### {title}\\n\")\n",
    "            f.write(f\" {url}\\n\")\n",
    "            f.write(f\"# Content:\\n{content}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u vÃ o '{filename}'\")\n",
    "\n",
    "# Cháº¡y chÃ­nh\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_site(BASE_URL, max_pages=30)\n",
    "    save_to_file(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35370648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/#page_title\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/#alert\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/history-center/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/sports-museum/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/fort-pitt/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/meadowcroft/\n",
      "ğŸ” Crawling: https://my.heinzhistorycenter.org/orders/558/calendar?cart=true&eventId=64babd55351d6b414477592b&membershipIds=\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/join/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/give/make-a-donation/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/search/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/#menu\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/heinz-history-center/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/sports-museum/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/fort-pitt/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/meadowcroft/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/history-center/exhibits/pittsburghs-hidden-history/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/event/smithsonian-lecture-series-preserving-the-space-age/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/events/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/event/remake-learning-days-handmade-museum-05162025/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/event/from-calabria-to-carnegie-music-for-bread/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/event/vulture-the-private-life-of-an-unloved-bird/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/event/italian-genealogy-workshop/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/detre-library-archives/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/exhibits/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/families/\n",
      "ğŸ” Crawling: https://shop.heinzhistorycenter.org/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/the-smithsonians-home-in-pittsburgh/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/explore/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/contact-us/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/accessibility/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/health-safety/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/work-with-us/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/sitemap/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/policies/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#224b4c444d624a474b4c584a4b51564d505b41474c5647500c4d5045\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#5b32353d341b333e3235213332282f342922383e352f3e297534293c\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#6c0a031e181c05181805020a032c040905021604051f18031e150f090218091e42031e0b\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#e5888084818a9286978a83918c8b838aa58d808c8b9f8d8c96918a979c86808b918097cb8a9782\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/event-rentals/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/visit/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/whats-on/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/african-american-program/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/rauh-jewish-history-program-archives/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/italian-american-program/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/publications/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/research/contribute-to-our-collections/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/give/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/give/fundraising-events/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/give/commemorative-gifts/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/give/planned-giving/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/learn/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/learn/education/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/learn/scout-programs/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/learn/america-101/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/learn/virtual-tours/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/blog/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/learn/videos-and-podcasts/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/people/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/press-awards-honors/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/about/history-center-affiliates-program/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/groups/\n",
      "ğŸ” Crawling: https://www.heinzhistorycenter.org/members/\n",
      "âœ… ÄÃ£ lÆ°u 65 trang vÃ o heinz_history_data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://www.heinzhistorycenter.org/\"\n",
    "visited = set()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc.endswith(\"heinzhistorycenter.org\")\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        res.raise_for_status()\n",
    "        return BeautifulSoup(res.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ KhÃ´ng thá»ƒ truy cáº­p {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl(url, depth=0, max_depth=2):\n",
    "    if url in visited or depth > max_depth:\n",
    "        return []\n",
    "\n",
    "    visited.add(url)\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    data = []\n",
    "    print(f\"ğŸ” Crawling: {url}\")\n",
    "\n",
    "    # Láº¥y ná»™i dung vÄƒn báº£n\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    data.append(f\"# URL: {url}\\n## Content:\\n{content}\\n{'='*80}\")\n",
    "\n",
    "    # Äá»‡ quy qua cÃ¡c link ná»™i bá»™\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = urljoin(url, link[\"href\"])\n",
    "        if is_valid_url(href):\n",
    "            data.extend(crawl(href, depth + 1, max_depth))\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_file(data, filename=\"heinz_history_data.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(data))\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u {len(data)} trang vÃ o {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = crawl(BASE_URL, max_depth=1)  # TÄƒng depth náº¿u muá»‘n crawl sÃ¢u hÆ¡n\n",
    "    save_to_file(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a69c2c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/#top\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/stories\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/calendar\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/equity\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/membership\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/rentals\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/cafe\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/store\n",
      "ğŸ” Crawling: https://tickets.thefrickpittsburgh.org/MembershipAndDonations.aspx?RegularOnly=True\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/plan-your-visit\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/plan-your-visit#visit\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/plan-your-visit#direction\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/Files/Admin/TheFrickPittsburghSiteMap2022.pdf?d=2022727165656\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/accessibility\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/tours\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/groupvisits\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/learn#education\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/plan-your-visit#plan\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/see-and-do\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p https://www.thefrickpittsburgh.org/see-and-do#box: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Read timed out. (read timeout=10)\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p https://www.thefrickpittsburgh.org/see-and-do#hap: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Read timed out. (read timeout=10)\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p https://www.thefrickpittsburgh.org/see-and-do#prog: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Read timed out. (read timeout=10)\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p https://www.thefrickpittsburgh.org/see-and-do#you: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Max retries exceeded with url: /see-and-do (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8D6ED2A0>, 'Connection to www.thefrickpittsburgh.org timed out. (connect timeout=10)'))\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/see-and-do#planvisit\n",
      "âš ï¸ KhÃ´ng thá»ƒ truy cáº­p http://www.thefrickpittsburgh.org/learn: HTTPConnectionPool(host='www.thefrickpittsburgh.org', port=80): Read timed out. (read timeout=10)\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/learn#fam\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/learn#work\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/learn#adult\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/learn#summer\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/learn#out\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/virtual\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/exhibitions\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/exhibitions\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/past-exhibitions\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/virtualexhibitions\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Exhibition-Kara-Walker-Harpers-Pictorial-History-of-the-Civil-War-Annotated\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/collection\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/support\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/the-frick-societies\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/reciprocal-museums\n",
      "ğŸ” Crawling: https://tickets.thefrickpittsburgh.org/Login.aspx\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/donate\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/LegacyPlanning\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/gifts-of-stock\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/Qualified-Charitable-Distributions\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/corporate-giving\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/eitc\n",
      "ğŸ” Crawling: http://www.thefrickpittsburgh.org/oursupporters\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/tickets\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Signature-Clayton-Tour-Experience-Gilded-Not-Golden\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/carandcarriagemuseum\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Exhibition-Catching-Sunbeams\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Summer-Fridays-at-the-Frick\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Exhibition-The-Scandinavian-Home-Landscape-and-Lore\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/greenhouse-gardens\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/plan-your-visit\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Duquesne-SoundWalk-at-the-Frick\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Car-and-Carriage-Museum\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Grow-Pittsburgh-Seedling-Sale\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-The-Frick-Art-Museum\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Remake-Learning-Day-at-the-Frick\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/Event-Sip-Make-Kara-Walker\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/mediainquiries\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/employment\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/programmatic-collaborations\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/mission\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/strategicplan\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/boardoftrustees\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/board-portal\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/leadership\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/landacknowledgement\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/pressreleases\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/photography-policy\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/familyandlegacy\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/historichome\n",
      "ğŸ” Crawling: https://www.thefrickpittsburgh.org/support-detail#contact\n",
      "âœ… ÄÃ£ lÆ°u 72 trang vÃ o Pittsburgh_frick_data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://www.thefrickpittsburgh.org/\"\n",
    "visited = set()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc.endswith(\"thefrickpittsburgh.org\")\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        return BeautifulSoup(res.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ KhÃ´ng thá»ƒ truy cáº­p {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl(url, depth=0, max_depth=1):\n",
    "    if url in visited or depth > max_depth:\n",
    "        return []\n",
    "\n",
    "    visited.add(url)\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    print(f\"ğŸ” Crawling: {url}\")\n",
    "    data = []\n",
    "\n",
    "    # Láº¥y ná»™i dung tá»« cÃ¡c tháº» Ä‘oáº¡n vÄƒn\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    data.append(f\"# URL: {url}\\n## Content:\\n{content}\\n{'='*80}\")\n",
    "\n",
    "    # Crawl thÃªm cÃ¡c link ná»™i bá»™\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = urljoin(url, link[\"href\"])\n",
    "        if is_valid_url(href):\n",
    "            data.extend(crawl(href, depth + 1, max_depth))\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_file(data, filename=\"Pittsburgh_frick_data.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(data))\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u {len(data)} trang vÃ o {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = crawl(BASE_URL, max_depth=1)\n",
    "    save_to_file(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e974b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u vÃ o 'pittsburgh_museums_wiki.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\"\n",
    "\n",
    "def get_page_content(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ KhÃ´ng thá»ƒ truy cáº­p {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(soup):\n",
    "    paragraphs = soup.select(\"p\")\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    return content\n",
    "\n",
    "def extract_tables(soup):\n",
    "    tables = soup.select(\"table.wikitable\")\n",
    "    extracted_tables = []\n",
    "\n",
    "    for table in tables:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        table_data = []\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all([\"th\", \"td\"])\n",
    "            cols_text = [col.get_text(strip=True) for col in cols]\n",
    "            if cols_text:\n",
    "                table_data.append(cols_text)\n",
    "\n",
    "        extracted_tables.append(table_data)\n",
    "    return extracted_tables\n",
    "\n",
    "def save_to_file(text, tables, filename=\"pittsburgh_museums_wiki.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ğŸ“„ Ná»™i dung vÄƒn báº£n:\\n\")\n",
    "        f.write(text + \"\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for i, table in enumerate(tables):\n",
    "            f.write(f\"ğŸ“Š Báº£ng {i+1}:\\n\")\n",
    "            for row in table:\n",
    "                f.write(\" | \".join(row) + \"\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u vÃ o '{filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    soup = get_page_content(URL)\n",
    "    if soup:\n",
    "        text = extract_text(soup)\n",
    "        tables = extract_tables(soup)\n",
    "        save_to_file(text, tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156a61b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Ghi vÃ o file duy nháº¥t\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ÄÃ£ hoÃ n táº¥t ghi toÃ n bá»™ header vÃ  paragraph vÃ o \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m link \u001b[38;5;241m=\u001b[39m urljoin(url, a_tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(link) \u001b[38;5;129;01mand\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m link \u001b[38;5;241m=\u001b[39m urljoin(url, a_tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(link) \u001b[38;5;129;01mand\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: crawl_page at line 52 (79 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m link \u001b[38;5;241m=\u001b[39m urljoin(url, a_tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(link) \u001b[38;5;129;01mand\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     21\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1368\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:317\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:278\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 278\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://bananasplitfest.com/\"\n",
    "DOMAIN = urlparse(BASE_URL).netloc\n",
    "visited = set()\n",
    "\n",
    "output_file = \"Banana Split Fest.txt\"\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return (parsed.scheme in [\"http\", \"https\"] and\n",
    "            (parsed.netloc == \"\" or DOMAIN in parsed.netloc))\n",
    "\n",
    "def crawl_page(url, file):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i khi truy cáº­p {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    body = soup.body\n",
    "    if not body:\n",
    "        return\n",
    "\n",
    "    file.write(f\"\\n{'='*80}\\nğŸ“Œ URL: {url}\\n\\n\")\n",
    "    \n",
    "    for element in body.descendants:\n",
    "        if element.name in [f\"h{i}\" for i in range(1, 7)]:\n",
    "            text = element.get_text(strip=True)\n",
    "            if text:\n",
    "                file.write(f\"## {text}\\n\")\n",
    "        elif element.name == \"p\":\n",
    "            text = element.get_text(strip=True)\n",
    "            if text:\n",
    "                file.write(f\"[CONTENT] {text}\\n\")\n",
    "\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        link = urljoin(url, a_tag['href'])\n",
    "        if is_valid_url(link) and link not in visited:\n",
    "            crawl_page(link, file)\n",
    "\n",
    "# Ghi vÃ o file duy nháº¥t\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    crawl_page(BASE_URL, f)\n",
    "\n",
    "print(f\"âœ… ÄÃ£ hoÃ n táº¥t ghi toÃ n bá»™ header vÃ  paragraph vÃ o {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
