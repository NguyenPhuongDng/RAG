{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50814017",
   "metadata": {},
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517aeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_wikipedia(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'id': 'bodyContent'})\n",
    "\n",
    "    if not content_div:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh.\")\n",
    "        return\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(allowed_tags):\n",
    "            # Xo√° ch√∫ th√≠ch trong th·∫ª <p>\n",
    "            if tag.name == 'p':\n",
    "                for sup in tag.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_wikipedia(\"https://en.wikipedia.org/wiki/Pittsburgh\", \"pittsburgh.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dda932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://en.wikipedia.org/wiki/Pittsburgh v√†o 'pittsburgh.txt'\n",
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://en.wikipedia.org/wiki/History_of_Pittsburgh v√†o 'history_of_pittsburgh.txt'\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://en.wikipedia.org/wiki/Pittsburgh\"\n",
    "url2 = \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\"\n",
    "output_file1 = \"pittsburgh.txt\"\n",
    "output_file2 = \"history_of_pittsburgh.txt\"\n",
    "\n",
    "crawl_wikipedia(url1, output_file1)\n",
    "crawl_wikipedia(url2, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780bdc",
   "metadata": {},
   "source": [
    "britannica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_britannica(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # C√°c th·∫ª mu·ªën gi·ªØ l·∫°i theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_britannica(\"https://www.britannica.com/place/Pittsburgh\", \"pittsburgh_britannica.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718a27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://www.britannica.com/place/Pittsburgh v√†o 'pittsburgh_britannica.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.britannica.com/place/Pittsburgh\"\n",
    "output_file = \"pittsburgh_britannica.txt\"\n",
    "crawl_britannica(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c3b5d",
   "metadata": {},
   "source": [
    "dynamic web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c5b3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def crawl_dynamic_site(url, output_file):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # ch·∫°y ·∫©n\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    service = Service(r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # ch·ªù trang load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_dynamic_site(\"https://www.visitpittsburgh.com/\", \"visitpittsburgh.txt\", driver_path=\"/path/to/chromedriver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4d6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://www.visitpittsburgh.com/ v√†o 'Pittsburgh webpage.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.visitpittsburgh.com/\"\n",
    "output_file = \"Pittsburgh webpage.txt\"\n",
    "crawl_dynamic_site(url=url, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "757f6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9622_amusement_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9626_payroll_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9623_isp_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9624_local_services_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9625_parking_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9627_uf_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: change-in-business-status-form-04.2025.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6492_2636_10_taxpayers_bill_of_rights_4-26-2018.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 16958_2022_tax_rate_by_tax_type.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 16957_2022_tax_due_date_calendar_.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8271_facility_usage_fee_information_for_performers_and_contracting_parties.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: firesale.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8398_payroll_expense_tax__et__allocation_schedule_form.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6825_payroll_expense_tax_allocation_schedule_for_professional_organization_form_instructions8.15.19.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8397_local_services_tax_ls-1_allocation_schedule_form.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6822_local_service_tax_allocation_schedule_for_professional_employer_organization_form_instructions8.15.19.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8403_general_contractor_detail_report.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 7065_general_contractor_detail_report_instructions.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: ls-tax-exemption-certificate-2025.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6819__lst_refund_form.pdf\n",
      "\n",
      "üìÑ ƒê√£ ghi to√†n b·ªô n·ªôi dung v√†o 'pittsburgh_tax.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def crawl_and_extract(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'class': 'main-container clearfix'})\n",
    "    if not content_div:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y v√πng n·ªôi dung ch√≠nh.\")\n",
    "        return\n",
    "\n",
    "    tags = ['h1', 'h2', 'h3', 'h4', 'p', 'a']\n",
    "    found_regulation = False\n",
    "    base_path = \"pdf_temp\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(tags):\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                text = tag.get_text(strip=True)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if tag.name == 'h1':\n",
    "                    f.write(f\"# {text}\\n\\n\")\n",
    "                elif tag.name == 'h2':\n",
    "                    f.write(f\"## {text}\\n\\n\")\n",
    "                elif tag.name == 'h3':\n",
    "                    f.write(f\"### {text}\\n\\n\")\n",
    "                elif tag.name == 'h4':\n",
    "                    f.write(f\"#### {text}\\n\\n\")\n",
    "                    if 'regulations' in text.lower():\n",
    "                        found_regulation = True\n",
    "                    else:\n",
    "                        found_regulation = False  # reset khi g·∫∑p h4 kh√°c\n",
    "\n",
    "            elif tag.name == 'p':\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    f.write(text + '\\n\\n')\n",
    "\n",
    "            elif tag.name == 'a' and found_regulation:\n",
    "                href = tag.get('href', '')\n",
    "                if href.endswith('.pdf'):\n",
    "                    pdf_url = urljoin(url, href)\n",
    "                    filename = os.path.basename(href)\n",
    "                    local_pdf = os.path.join(base_path, filename)\n",
    "\n",
    "                    try:\n",
    "                        pdf_res = requests.get(pdf_url, headers=headers)\n",
    "                        with open(local_pdf, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_res.content)\n",
    "                        f.write(f\"üìé N·ªôi dung t·ª´ file PDF: {filename}\\n\\n\")\n",
    "\n",
    "                        # Tr√≠ch n·ªôi dung PDF\n",
    "                        doc = fitz.open(local_pdf)\n",
    "                        for page in doc:\n",
    "                            text = page.get_text().strip()\n",
    "                            if text:\n",
    "                                f.write(text + '\\n\\n')\n",
    "                        doc.close()\n",
    "                        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: {filename}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"[L·ªói khi t·∫£i ho·∫∑c ƒë·ªçc PDF {filename}: {e}]\\n\\n\")\n",
    "\n",
    "    print(f\"\\nüìÑ ƒê√£ ghi to√†n b·ªô n·ªôi dung v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "crawl_and_extract(\n",
    "    \"https://www.pittsburghpa.gov/City-Government/Finances-Budget/Taxes/Tax-Forms\",\n",
    "    \"pittsburgh_tax.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e509a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n v√†o '2024 Operating Budget.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # ·∫®n c·∫£nh b√°o SSL\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url: str, output_txt: str = \"output.txt\"):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, verify=False)  # ‚ö†Ô∏è B·ªè qua SSL verify\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        doc = fitz.open(\"temp.pdf\")\n",
    "        all_text = []\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            # Gi·ªØ nguy√™n xu·ªëng d√≤ng, nh∆∞ng lo·∫°i kho·∫£ng tr·∫Øng th·ª´a\n",
    "            cleaned_lines = [line.strip() for line in page_text.splitlines() if line.strip()]\n",
    "            all_text.extend(cleaned_lines)\n",
    "            all_text.append(\"\")  # Th√™m d√≤ng tr·ªëng gi·ªØa c√°c trang (t√πy ch·ªçn)\n",
    "\n",
    "        # Ghi ra file, m·ªói d√≤ng l√† 1 d√≤ng vƒÉn b·∫£n r√µ r√†ng\n",
    "        with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_text))\n",
    "\n",
    "        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n v√†o '{output_txt}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói: {e}\")\n",
    "\n",
    "extract_text_from_pdf_url(\n",
    "     \"https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf\",\n",
    "     \"2024 Operating Budget.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung v√†o 'cmu_about.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = [\"https://www.cmu.edu/about/\",\n",
    "       \"https://www.cmu.edu/academics/interdisciplinary-programs.html\",\n",
    "        \"https://www.library.cmu.edu/\",\n",
    "        \"https://www.cmu.edu/academics/learning-for-a-lifetime.html\",\n",
    "        \"https://www.cmu.edu/admission/student-community-blog\",\n",
    "        \"https://www.cmu.edu/graduate/prospective/index.html\",\n",
    "        \"https://www.cmu.edu/leadership/\",\n",
    "        \"https://www.cmu.edu/about/mission.html\",\n",
    "        \"https://www.cmu.edu/about/history.html\",\n",
    "        \"https://www.cmu.edu/about/traditions.html\",\n",
    "        \"https://www.cmu.edu/inclusive-excellence/\",\n",
    "        \"https://www.cmu.edu/about/pittsburgh.html\",\n",
    "        \"https://www.cmu.edu/about/rankings.html\",\n",
    "        \"https://www.cmu.edu/about/awards.html\",\n",
    "        \"https://www.cmu.edu/visit//visitor-information\",\n",
    "        \"https://www.cmu.edu/research/centers-and-institutes.html\",\n",
    "        \"https://www.cmu.edu/student-experience/index.html\",\n",
    "        ]\n",
    "\n",
    "def crawl_cmu_about():\n",
    "    url = \"https://www.cmu.edu/about/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # B√°o l·ªói n·∫øu kh√¥ng truy c·∫≠p ƒë∆∞·ª£c\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # L·∫•y n·ªôi dung ch√≠nh trong ph·∫ßn <main> ho·∫∑c v√πng ch√≠nh\n",
    "    main_content = soup.find(\"main\")\n",
    "    if not main_content:\n",
    "        main_content = soup.body  # fallback n·∫øu kh√¥ng c√≥ <main>\n",
    "\n",
    "    # Lo·∫°i b·ªè script, style, nav,...\n",
    "    for tag in main_content([\"script\", \"style\", \"nav\", \"footer\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = main_content.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    clean_text = \"\\n\".join(lines)\n",
    "\n",
    "    # Ghi v√†o file\n",
    "    with open(\"cmu_about.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(\"‚úÖ ƒê√£ l∆∞u n·ªôi dung v√†o 'cmu_about.txt'\")\n",
    "\n",
    "crawl_cmu_about()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52faff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/cmu-fact-sheet.pdf\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/awards.html\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/index.html\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/rankings.html\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/mission.html\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/history.html\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/traditions.html\n",
      "‚úÖ ƒê√£ l∆∞u: https://www.cmu.edu/about/pittsburgh.html\n",
      "‚ùå L·ªói truy c·∫≠p https://www.cmu.edu/about/pittsburgh-old.html: 404 Client Error: Not Found for url: https://www.cmu.edu/about/pittsburgh-old.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "\n",
    "base_url = \"https://www.cmu.edu/about/\"\n",
    "visited = set()\n",
    "\n",
    "def clean_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    main = soup.find(\"main\") or soup.body\n",
    "\n",
    "    if main is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Lo·∫°i b·ªè tag kh√¥ng c·∫ßn thi·∫øt\n",
    "    for tag in main.find_all([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = main.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def save_text(url, text, folder=\"cmu_about_pages\"):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    path = urlparse(url).path.strip(\"/\").replace(\"/\", \"_\")\n",
    "    filename = \"index\" if path == \"\" else path\n",
    "    with open(f\"{folder}/{filename}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def crawl(url):\n",
    "    if url in visited or not url.startswith(base_url):\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói truy c·∫≠p {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    text = clean_text(response.text)\n",
    "    save_text(url, text)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u: {url}\")\n",
    "\n",
    "    # Ti·∫øp t·ª•c crawl c√°c li√™n k·∫øt n·ªôi b·ªô\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "        full_url = urljoin(url, href)\n",
    "        if full_url.startswith(base_url):\n",
    "            crawl(full_url)\n",
    "\n",
    "crawl(base_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
