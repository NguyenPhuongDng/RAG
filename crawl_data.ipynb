{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50814017",
   "metadata": {},
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517aeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_wikipedia(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'id': 'bodyContent'})\n",
    "\n",
    "    if not content_div:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh.\")\n",
    "        return\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(allowed_tags):\n",
    "            # Xo√° ch√∫ th√≠ch trong th·∫ª <p>\n",
    "            if tag.name == 'p':\n",
    "                for sup in tag.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_wikipedia(\"https://en.wikipedia.org/wiki/Pittsburgh\", \"pittsburgh.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dda932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://en.wikipedia.org/wiki/Pittsburgh v√†o 'pittsburgh.txt'\n",
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://en.wikipedia.org/wiki/History_of_Pittsburgh v√†o 'history_of_pittsburgh.txt'\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://en.wikipedia.org/wiki/Pittsburgh\"\n",
    "url2 = \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\"\n",
    "output_file1 = \"pittsburgh.txt\"\n",
    "output_file2 = \"history_of_pittsburgh.txt\"\n",
    "\n",
    "crawl_wikipedia(url1, output_file1)\n",
    "crawl_wikipedia(url2, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780bdc",
   "metadata": {},
   "source": [
    "britannica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_britannica(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # C√°c th·∫ª mu·ªën gi·ªØ l·∫°i theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_britannica(\"https://www.britannica.com/place/Pittsburgh\", \"pittsburgh_britannica.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718a27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://www.britannica.com/place/Pittsburgh v√†o 'pittsburgh_britannica.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.britannica.com/place/Pittsburgh\"\n",
    "output_file = \"pittsburgh_britannica.txt\"\n",
    "crawl_britannica(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c3b5d",
   "metadata": {},
   "source": [
    "dynamic web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def crawl_dynamic_site(url, output_file):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # ch·∫°y ·∫©n\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    service = Service(r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # ch·ªù trang load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_dynamic_site(\"https://www.visitpittsburgh.com/\", \"visitpittsburgh.txt\", driver_path=\"/path/to/chromedriver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4d6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://www.visitpittsburgh.com/ v√†o 'Pittsburgh webpage.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.visitpittsburgh.com/\"\n",
    "output_file = \"Pittsburgh webpage.txt\"\n",
    "crawl_dynamic_site(url=url, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "757f6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9622_amusement_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9626_payroll_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9623_isp_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9624_local_services_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9625_parking_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9627_uf_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: change-in-business-status-form-04.2025.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6492_2636_10_taxpayers_bill_of_rights_4-26-2018.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 16958_2022_tax_rate_by_tax_type.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 16957_2022_tax_due_date_calendar_.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8271_facility_usage_fee_information_for_performers_and_contracting_parties.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: firesale.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8398_payroll_expense_tax__et__allocation_schedule_form.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6825_payroll_expense_tax_allocation_schedule_for_professional_organization_form_instructions8.15.19.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8397_local_services_tax_ls-1_allocation_schedule_form.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6822_local_service_tax_allocation_schedule_for_professional_employer_organization_form_instructions8.15.19.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8403_general_contractor_detail_report.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 7065_general_contractor_detail_report_instructions.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: ls-tax-exemption-certificate-2025.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6819__lst_refund_form.pdf\n",
      "\n",
      "üìÑ ƒê√£ ghi to√†n b·ªô n·ªôi dung v√†o 'pittsburgh_tax.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def crawl_and_extract(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'class': 'main-container clearfix'})\n",
    "    if not content_div:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y v√πng n·ªôi dung ch√≠nh.\")\n",
    "        return\n",
    "\n",
    "    tags = ['h1', 'h2', 'h3', 'h4', 'p', 'a']\n",
    "    found_regulation = False\n",
    "    base_path = \"pdf_temp\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(tags):\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                text = tag.get_text(strip=True)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if tag.name == 'h1':\n",
    "                    f.write(f\"# {text}\\n\\n\")\n",
    "                elif tag.name == 'h2':\n",
    "                    f.write(f\"## {text}\\n\\n\")\n",
    "                elif tag.name == 'h3':\n",
    "                    f.write(f\"### {text}\\n\\n\")\n",
    "                elif tag.name == 'h4':\n",
    "                    f.write(f\"#### {text}\\n\\n\")\n",
    "                    if 'regulations' in text.lower():\n",
    "                        found_regulation = True\n",
    "                    else:\n",
    "                        found_regulation = False  # reset khi g·∫∑p h4 kh√°c\n",
    "\n",
    "            elif tag.name == 'p':\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    f.write(text + '\\n\\n')\n",
    "\n",
    "            elif tag.name == 'a' and found_regulation:\n",
    "                href = tag.get('href', '')\n",
    "                if href.endswith('.pdf'):\n",
    "                    pdf_url = urljoin(url, href)\n",
    "                    filename = os.path.basename(href)\n",
    "                    local_pdf = os.path.join(base_path, filename)\n",
    "\n",
    "                    try:\n",
    "                        pdf_res = requests.get(pdf_url, headers=headers)\n",
    "                        with open(local_pdf, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_res.content)\n",
    "                        f.write(f\"üìé N·ªôi dung t·ª´ file PDF: {filename}\\n\\n\")\n",
    "\n",
    "                        # Tr√≠ch n·ªôi dung PDF\n",
    "                        doc = fitz.open(local_pdf)\n",
    "                        for page in doc:\n",
    "                            text = page.get_text().strip()\n",
    "                            if text:\n",
    "                                f.write(text + '\\n\\n')\n",
    "                        doc.close()\n",
    "                        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: {filename}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"[L·ªói khi t·∫£i ho·∫∑c ƒë·ªçc PDF {filename}: {e}]\\n\\n\")\n",
    "\n",
    "    print(f\"\\nüìÑ ƒê√£ ghi to√†n b·ªô n·ªôi dung v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "crawl_and_extract(\n",
    "    \"https://www.pittsburghpa.gov/City-Government/Finances-Budget/Taxes/Tax-Forms\",\n",
    "    \"pittsburgh_tax.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e509a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n v√†o '2024 Operating Budget.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # ·∫®n c·∫£nh b√°o SSL\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url: str, output_txt: str = \"output.txt\"):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, verify=False)  # ‚ö†Ô∏è B·ªè qua SSL verify\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        doc = fitz.open(\"temp.pdf\")\n",
    "        all_text = []\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            # Gi·ªØ nguy√™n xu·ªëng d√≤ng, nh∆∞ng lo·∫°i kho·∫£ng tr·∫Øng th·ª´a\n",
    "            cleaned_lines = [line.strip() for line in page_text.splitlines() if line.strip()]\n",
    "            all_text.extend(cleaned_lines)\n",
    "            all_text.append(\"\")  # Th√™m d√≤ng tr·ªëng gi·ªØa c√°c trang (t√πy ch·ªçn)\n",
    "\n",
    "        # Ghi ra file, m·ªói d√≤ng l√† 1 d√≤ng vƒÉn b·∫£n r√µ r√†ng\n",
    "        with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_text))\n",
    "\n",
    "        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n v√†o '{output_txt}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói: {e}\")\n",
    "\n",
    "extract_text_from_pdf_url(\n",
    "     \"https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf\",\n",
    "     \"2024 Operating Budget.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb5fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawled: https://www.cmu.edu/about/\n",
      "‚úÖ Crawled: https://www.cmu.edu/academics/interdisciplinary-programs.html\n",
      "‚úÖ Crawled: https://www.library.cmu.edu/\n",
      "‚úÖ Crawled: https://www.cmu.edu/academics/learning-for-a-lifetime.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/admission/student-community-blog\n",
      "‚úÖ Crawled: https://www.cmu.edu/graduate/prospective/index.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/leadership/\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/mission.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/history.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/traditions.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/inclusive-excellence/\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/pittsburgh.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/rankings.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/awards.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/visit//visitor-information\n",
      "‚úÖ Crawled: https://www.cmu.edu/research/centers-and-institutes.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/student-experience/index.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "urls = [\n",
    "    \"https://www.cmu.edu/about/\",\n",
    "    \"https://www.cmu.edu/academics/interdisciplinary-programs.html\",\n",
    "    \"https://www.library.cmu.edu/\",\n",
    "    \"https://www.cmu.edu/academics/learning-for-a-lifetime.html\",\n",
    "    \"https://www.cmu.edu/admission/student-community-blog\",\n",
    "    \"https://www.cmu.edu/graduate/prospective/index.html\",\n",
    "    \"https://www.cmu.edu/leadership/\",\n",
    "    \"https://www.cmu.edu/about/mission.html\",\n",
    "    \"https://www.cmu.edu/about/history.html\",\n",
    "    \"https://www.cmu.edu/about/traditions.html\",\n",
    "    \"https://www.cmu.edu/inclusive-excellence/\",\n",
    "    \"https://www.cmu.edu/about/pittsburgh.html\",\n",
    "    \"https://www.cmu.edu/about/rankings.html\",\n",
    "    \"https://www.cmu.edu/about/awards.html\",\n",
    "    \"https://www.cmu.edu/visit//visitor-information\",\n",
    "    \"https://www.cmu.edu/research/centers-and-institutes.html\",\n",
    "    \"https://www.cmu.edu/student-experience/index.html\",\n",
    "]\n",
    "\n",
    "def extract_header(url):\n",
    "    # L·∫•y ph·∫ßn sau c√πng c·ªßa URL ƒë·ªÉ l√†m ti√™u ƒë·ªÅ\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip(\"/\").split(\"/\")[-1]\n",
    "    if not path or path == \"index.html\":\n",
    "        path = parsed.path.strip(\"/\").split(\"/\")[-2] if len(parsed.path.strip(\"/\").split(\"/\")) > 1 else \"home\"\n",
    "    path = re.sub(r'\\.html$', '', path)\n",
    "    return \"#\" + path.lower().replace(\" \", \"-\")\n",
    "\n",
    "def clean_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    main = soup.find(\"main\") or soup.body\n",
    "\n",
    "    if main is None:\n",
    "        return \"\"\n",
    "\n",
    "    for tag in main.find_all([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = main.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def crawl_and_write(urls, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                print(f\"‚úÖ Crawled: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói truy c·∫≠p {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            header = extract_header(url)\n",
    "            text = clean_text(response.text)\n",
    "\n",
    "            if text:\n",
    "                f.write(header + \"\\n\")\n",
    "                f.write(text + \"\\n\\n\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ·ªü {url}\")\n",
    "\n",
    "# G·ªçi h√†m crawl\n",
    "crawl_and_write(urls, \"cmu_about.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027b9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang crawl trang 1...\n",
      "üîç ƒêang crawl trang 2...\n",
      "üîç ƒêang crawl trang 3...\n",
      "üîç ƒêang crawl trang 4...\n",
      "üîç ƒêang crawl trang 5...\n",
      "üîç ƒêang crawl trang 6...\n",
      "üîç ƒêang crawl trang 7...\n",
      "üîç ƒêang crawl trang 8...\n",
      "üîç ƒêang crawl trang 9...\n",
      "üîç ƒêang crawl trang 10...\n",
      "üîç ƒêang crawl trang 11...\n",
      "üîç ƒêang crawl trang 12...\n",
      "üîç ƒêang crawl trang 13...\n",
      "üîç ƒêang crawl trang 14...\n",
      "üîç ƒêang crawl trang 15...\n",
      "üîç ƒêang crawl trang 16...\n",
      "üîç ƒêang crawl trang 17...\n",
      "üîç ƒêang crawl trang 18...\n",
      "üîç ƒêang crawl trang 19...\n",
      "üîç ƒêang crawl trang 20...\n",
      "üîç ƒêang crawl trang 21...\n",
      "üîç ƒêang crawl trang 22...\n",
      "üîç ƒêang crawl trang 23...\n",
      "üîç ƒêang crawl trang 24...\n",
      "üîç ƒêang crawl trang 25...\n",
      "üîç ƒêang crawl trang 26...\n",
      "üîç ƒêang crawl trang 27...\n",
      "üîç ƒêang crawl trang 28...\n",
      "üîç ƒêang crawl trang 29...\n",
      "üîç ƒêang crawl trang 30...\n",
      "üîç ƒêang crawl trang 31...\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o 'Downtown Pittsburgh events calendar.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_all_events(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 32):  # t·ª´ 1 ƒë·∫øn 31\n",
    "            url = f\"https://downtownpittsburgh.com/events/?n=5&d={page}&y=2025\"\n",
    "            print(f\"üîç ƒêang crawl trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            h1_tags = soup.find_all('h1')\n",
    "\n",
    "            if not h1_tags:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán tr√™n trang {page}\")\n",
    "                continue\n",
    "\n",
    "            for h1 in h1_tags:\n",
    "                link_tag = h1.find('a')\n",
    "                if not link_tag:\n",
    "                    continue\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                href = link_tag.get('href', '')\n",
    "                full_url = 'https://downtownpittsburgh.com' + href\n",
    "\n",
    "                sibling = h1.next_sibling\n",
    "                date_text = None\n",
    "                description_parts = []\n",
    "\n",
    "                while sibling:\n",
    "                    if not hasattr(sibling, 'get_text'):\n",
    "                        sibling = sibling.next_sibling\n",
    "                        continue\n",
    "\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text:\n",
    "                        if not date_text and ('am' in text.lower() or 'pm' in text.lower() or '-' in text):\n",
    "                            date_text = text\n",
    "                        elif text == \"READ MORE\":\n",
    "                            break\n",
    "                        else:\n",
    "                            description_parts.append(text)\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "                if not date_text:\n",
    "                    date_text = \"No date\"\n",
    "                description = \" \".join(description_parts).strip() if description_parts else \"No description\"\n",
    "\n",
    "                f.write(f\"## [{title}]({full_url})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date_text}\\n\")\n",
    "                f.write(f\"**Description**: {description}\\n\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o '{output_file}'\")\n",
    "\n",
    "\n",
    "crawl_all_events(\"Downtown Pittsburgh events calendar.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "477a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ƒêang x·ª≠ l√Ω trang 1...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 2...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 3...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 4...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 5...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 6...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 7...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 8...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 9...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 10...\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u s·ª± ki·ªán v√†o 'citypaper_events.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_citypaper_detailed(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 11):\n",
    "            url = f\"https://www.pghcitypaper.com/pittsburgh/EventSearch?page={page}&v=d\"\n",
    "            print(f\"üîé ƒêang x·ª≠ l√Ω trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            events = soup.find_all('p', class_='fdn-teaser-headline')\n",
    "\n",
    "            for headline in events:\n",
    "                # L·∫•y ti√™u ƒë·ªÅ v√† link\n",
    "                a_tag = headline.find('a', href=True)\n",
    "                if not a_tag:\n",
    "                    continue\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                link = a_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://www.pghcitypaper.com' + link\n",
    "\n",
    "                # T√¨m c√°c ph·∫ßn c√≤n l·∫°i th√¥ng qua cha c·ªßa headline\n",
    "                parent = headline.find_parent()\n",
    "                date_tag = parent.find_next('p', class_='fdn-teaser-subheadline')\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"No date\"\n",
    "\n",
    "                loc_tag = parent.find_next('a', class_='fdn-event-teaser-location-link')\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else \"No location\"\n",
    "\n",
    "                addr_tag = parent.find_next('p', class_='fdn-inline-split-list')\n",
    "                address = addr_tag.get_text(strip=True) if addr_tag else \"No address\"\n",
    "\n",
    "                desc_tag = parent.find_next('div', class_='fdn-teaser-description')\n",
    "                desc = desc_tag.get_text(strip=True) if desc_tag else \"No description\"\n",
    "\n",
    "                # Ghi ra file\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date}\\n\")\n",
    "                f.write(f\"**Location**: {location}, {address}\\n\")\n",
    "                f.write(f\"**Description**: {desc}\\n\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ ƒê√£ l∆∞u s·ª± ki·ªán v√†o '{output_file}'\")\n",
    "\n",
    "# G·ªçi h√†m:\n",
    "crawl_citypaper_detailed(\"citypaper_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6386f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250401\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250402\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250403\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250404\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250405\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250406\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250407\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250408\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250409\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250410\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250411\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250412\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250413\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250414\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250415\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250416\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250417\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250418\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250419\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250420\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250421\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250422\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250423\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250424\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250425\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250426\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250427\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250428\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250429\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250430\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250501\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250502\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250503\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250504\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250505\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250506\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250507\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250508\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250509\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250510\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250511\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250512\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250513\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o 'cmu_events.txt'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def crawl_cmu_events_selenium(start_date, end_date, output_file):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # kh√¥ng m·ªü c·ª≠a s·ªï tr√¨nh duy·ªát\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime('%Y%m%d')\n",
    "            url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "            print(f\"üîé ƒêang x·ª≠ l√Ω: {url}\")\n",
    "\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # ƒë·ª£i trang load JS xong\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            events = soup.find_all('div', class_='lw_cal_event_info')\n",
    "\n",
    "            if not events:\n",
    "                current_date += timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            for event in events:\n",
    "                title_tag = event.find('div', class_='lw_events_title')\n",
    "                title = title_tag.a.get_text(strip=True) if title_tag and title_tag.a else \"Kh√¥ng c√≥ ti√™u ƒë·ªÅ\"\n",
    "                link = title_tag.a['href'] if title_tag and title_tag.a else \"#\"\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://events.cmu.edu' + link\n",
    "\n",
    "                location = event.find('div', class_='lw_events_location')\n",
    "                time_tag = event.find('div', class_='lw_events_time')\n",
    "                summary = event.find('div', class_='lw_events_summary')\n",
    "\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date**: {current_date.strftime('%Y-%m-%d')}\\n\")\n",
    "                f.write(f\"**Time**: {time_tag.get_text(strip=True) if time_tag else 'Kh√¥ng r√µ'}\\n\")\n",
    "                f.write(f\"**Location**: {location.get_text(strip=True) if location else 'Kh√¥ng r√µ'}\\n\")\n",
    "                f.write(f\"**Summary**: {summary.get_text(strip=True) if summary else 'Kh√¥ng c√≥'}\\n\\n\")\n",
    "\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\n‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o '{output_file}'\")\n",
    "\n",
    "# G·ªçi h√†m\n",
    "start = datetime(2025, 4, 1)\n",
    "end = datetime(2025, 5, 13)\n",
    "crawl_cmu_events_selenium(start, end, \"cmu_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5f43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán n√†o. Vui l√≤ng ki·ªÉm tra file crawl_log.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# C·∫•u h√¨nh logging\n",
    "logging.basicConfig(filename='crawl_log.txt', level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† kho·∫£ng tr·∫Øng th·ª´a\n",
    "    return re.sub(r'\\s+', ' ', text.strip()) if text else 'N/A'\n",
    "\n",
    "def crawl_events():\n",
    "    url = \"https://www.visitpittsburgh.com/events-festivals/?page=11\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"ƒêang g·ª≠i y√™u c·∫ßu t·ªõi {url}\")\n",
    "        # G·ª≠i y√™u c·∫ßu GET\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        logging.info(\"Y√™u c·∫ßu th√†nh c√¥ng\")\n",
    "        \n",
    "        # Ph√¢n t√≠ch HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        logging.info(\"ƒê√£ ph√¢n t√≠ch HTML\")\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ c√°c s·ª± ki·ªán\n",
    "        events = soup.find_all('div', class_='card card--common card--listing')\n",
    "        logging.info(f\"T√¨m th·∫•y {len(events)} s·ª± ki·ªán\")\n",
    "        \n",
    "        if not events:\n",
    "            logging.warning(\"Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán n√†o. Ki·ªÉm tra c·∫•u tr√∫c HTML.\")\n",
    "            print(\"Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán n√†o. Vui l√≤ng ki·ªÉm tra file crawl_log.txt\")\n",
    "            return\n",
    "        \n",
    "        # M·ªü file ƒë·ªÉ l∆∞u d·ªØ li·ªáu\n",
    "        with open('pittsburgh_events.txt', 'w', encoding='utf-8') as f:\n",
    "            for i, event in enumerate(events, 1):\n",
    "                try:\n",
    "                    # L·∫•y ti√™u ƒë·ªÅ\n",
    "                    title_elem = event.find('a', class_='card_heading')\n",
    "                    title = clean_text(title_elem.text)\n",
    "                    \n",
    "                    # L·∫•y ng√†y\n",
    "                    date_elem = event.find('p', class_='date-heading card_date-heading')\n",
    "                    date = clean_text(date_elem.text)\n",
    "                    \n",
    "                    # L·∫•y ƒë·ªãa ch·ªâ\n",
    "                    address_elem = event.find('div', class_='card____address')\n",
    "                    address = clean_text(address_elem.text)\n",
    "                    \n",
    "                    # L·∫•y s·ªë ƒëi·ªán tho·∫°i\n",
    "                    phone_elem = event.find('div', class_='card____phone')\n",
    "                    phone = clean_text(phone_elem.text)\n",
    "                    \n",
    "                    # Ghi v√†o file\n",
    "                    f.write(f\"Title: {title}\\n\")\n",
    "                    f.write(f\"Date: {date}\\n\")\n",
    "                    f.write(f\"Address: {address}\\n\")\n",
    "                    f.write(f\"Phone: {phone}\\n\")\n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                    \n",
    "                    logging.info(f\"ƒê√£ crawl s·ª± ki·ªán {i}: {title}\")\n",
    "                    \n",
    "                except AttributeError as e:\n",
    "                    logging.error(f\"L·ªói khi crawl s·ª± ki·ªán {i}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o pittsburgh_events.txt\")\n",
    "        logging.info(\"Ho√†n t·∫•t crawl d·ªØ li·ªáu\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"L·ªói khi truy c·∫≠p trang web: {e}\")\n",
    "        logging.error(f\"L·ªói khi truy c·∫≠p trang web: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√°c: {e}\")\n",
    "        logging.error(f\"L·ªói kh√°c: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0a50244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªói khi truy c·∫≠p https://carnegiemuseums.org/: 403 Client Error: Forbidden for url: https://carnegiemuseums.org/\n",
      "‚úÖ ƒê√£ crawl v√† ghi d·ªØ li·ªáu v√†o cmu_events.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def extract_event_info(soup):\n",
    "    title = soup.find('h1')\n",
    "    title_text = title.get_text(strip=True) if title else \"UnKnown Title\"\n",
    "\n",
    "    # T√¨m ng√†y n·∫øu c√≥ trong ƒëo·∫°n vƒÉn ho·∫∑c th·∫ª time\n",
    "    time_text = \"\"\n",
    "    time_tag = soup.find('time')\n",
    "    if time_tag:\n",
    "        time_text = time_tag.get_text(strip=True)\n",
    "    else:\n",
    "        # T√¨m chu·ªói gi·ªëng ng√†y th√°ng\n",
    "        text_block = soup.get_text()\n",
    "        match = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(,)?\\s+\\d{4}', text_block)\n",
    "        if match:\n",
    "            time_text = match.group(0)\n",
    "\n",
    "    # T√¨m m√¥ t·∫£\n",
    "    desc_div = soup.find('div', class_='content')\n",
    "    description = desc_div.get_text(separator='\\n', strip=True) if desc_div else \"\"\n",
    "\n",
    "    # N·∫øu kh√¥ng c√≥ m√¥ t·∫£, l·∫•y ƒëo·∫°n <p> ƒë·∫ßu ti√™n\n",
    "    if not description:\n",
    "        p_tag = soup.find('p')\n",
    "        if p_tag:\n",
    "            description = p_tag.get_text(strip=True)\n",
    "\n",
    "    return title_text, time_text, description\n",
    "\n",
    "def crawl(url, depth=0, max_depth=2, file=None):\n",
    "    if url in visited or depth > max_depth:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title, time_text, description = extract_event_info(soup)\n",
    "    if title or description:\n",
    "        file.write(f\"URL: {url}\\n\")\n",
    "        file.write(f\"Title: {title}\\n\")\n",
    "        if time_text:\n",
    "            file.write(f\"Time: {time_text}\\n\")\n",
    "        file.write(f\"Describe:\\n{description}\\n\")\n",
    "        file.write(\"-\" * 100 + \"\\n\")\n",
    "\n",
    "    # ƒê·ªá quy v√†o c√°c link n·ªôi b·ªô li√™n quan\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        full_url = urljoin(url, href)\n",
    "        parsed = urlparse(full_url)\n",
    "\n",
    "        # Ch·ªâ crawl c√°c trang c·ªßa cmu.edu v√† ch·ª©a t·ª´ kh√≥a li√™n quan\n",
    "        if ('cmu.edu' in parsed.netloc) and ('events' in parsed.path):\n",
    "            crawl(full_url, depth + 1, max_depth, file)\n",
    "\n",
    "# G·ªçi crawl v√† ghi d·ªØ li·ªáu\n",
    "with open(\"campus events page.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    start_url = \"https://carnegiemuseums.org/\"\n",
    "    crawl(start_url, max_depth=20, file=f)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ crawl v√† ghi d·ªØ li·ªáu v√†o cmu_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9b7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://classicalvoiceamerica.org/2025/04/29/motherhoods-painful-art-splashed-on-intimate-canvas-of-chamber/: 403 Client Error: Forbidden for url: https://classicalvoiceamerica.org/2025/04/29/motherhoods-painful-art-splashed-on-intimate-canvas-of-chamber/\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://pittsburgh.tablemagazine.com/blog/woman-with-eyes-closed/: 403 Client Error: Forbidden for url: https://pittsburgh.tablemagazine.com/blog/woman-with-eyes-closed/\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/theater-dance/2025/01/27/pittsburgh-opera-armida-review-tickets-resident-artists/stories/202501270050: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/theater-dance/2025/01/27/pittsburgh-opera-armida-review-tickets-resident-artists/stories/202501270050 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8BEF6530>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2025/01/22/music-101-mozart-vs-bach/stories/202501170056: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2025/01/22/music-101-mozart-vs-bach/stories/202501170056 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8BEF4490>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/11/11/pittsburgh-opera-review-cavalleria-rusticana-pagliacci-tickets-chorus/stories/202411110068: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/11/11/pittsburgh-opera-review-cavalleria-rusticana-pagliacci-tickets-chorus/stories/202411110068 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B18C8B0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/10/31/pittsburgh-opera-chorus-audition-rehearsal-choir-singer/stories/202410300080: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/10/31/pittsburgh-opera-chorus-audition-rehearsal-choir-singer/stories/202410300080 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B4F8790>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/10/08/pittsburgh-opera-tosca-review-puccini/stories/202410070079: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/10/08/pittsburgh-opera-tosca-review-puccini/stories/202410070079 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8D579270>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/10/02/pittsburgh-opera-tosca-tickets-music-101/stories/202409260106?utm_campaign=Newsletter%20Editorial%20Calendar%202024-25%20Season&utm_medium=email&_hsenc=p2ANqtz-8O7I6kgxBIX8eLhOCjCcgQaEJbBe9bluEnPivxnIueXI2ZoH9Hb5Q0UcFDYbud02GmCXnp_m4ST1LWh_YhRU596_Tb-dnMB2ByxYUfMN2nPhokmOA&_hsmi=2&utm_content=2&utm_source=hs_email: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/10/02/pittsburgh-opera-tosca-tickets-music-101/stories/202409260106?utm_campaign=Newsletter%20Editorial%20Calendar%202024-25%20Season&utm_medium=email&_hsenc=p2ANqtz-8O7I6kgxBIX8eLhOCjCcgQaEJbBe9bluEnPivxnIueXI2ZoH9Hb5Q0UcFDYbud02GmCXnp_m4ST1LWh_YhRU596_Tb-dnMB2ByxYUfMN2nPhokmOA&_hsmi=2&utm_content=2&utm_source=hs_email (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B73AE60>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/07/25/pittsburgh-opera-conductor-antony-walker-contract/stories/202407230067: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/07/25/pittsburgh-opera-conductor-antony-walker-contract/stories/202407230067 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8D0EC1C0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/05/02/pittsburgh-opera-passion-mary-cardwell-dawson-national-negro-company/stories/202405010083: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/05/02/pittsburgh-opera-passion-mary-cardwell-dawson-national-negro-company/stories/202405010083 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8DBDD750>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://triblive.com/aande/theater-arts/pittsburgh-operas-passion-of-mary-cardwell-dawson-is-a-performance-to-remember/: 403 Client Error: Forbidden for url: https://triblive.com/aande/theater-arts/pittsburgh-operas-passion-of-mary-cardwell-dawson-is-a-performance-to-remember/\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://operawire.com/five-winners-of-metropolitan-operas-2024-laffont-competition-announced/: 403 Client Error: Forbidden for url: https://operawire.com/five-winners-of-metropolitan-operas-2024-laffont-competition-announced/\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2024/01/16/pittsburgh-opera-iphigenie-tickets-baroque-inflation-set-cost-steven-grair/stories/202401100101: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2024/01/16/pittsburgh-opera-iphigenie-tickets-baroque-inflation-set-cost-steven-grair/stories/202401100101 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B4FB5E0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/2023/11/07/pittsburgh-opera-flying-dutchman-tickets-wagner-sam-helfrich-ghost-ship/stories/202311030067: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/2023/11/07/pittsburgh-opera-flying-dutchman-tickets-wagner-sam-helfrich-ghost-ship/stories/202311030067 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8DCDA080>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link https://www.post-gazette.com/ae/music/2023/10/15/pittsburgh-opera-review-barber-of-seville-tickets-figaro-season/stories/202310150123: HTTPSConnectionPool(host='www.post-gazette.com', port=443): Max retries exceeded with url: /ae/music/2023/10/15/pittsburgh-opera-review-barber-of-seville-tickets-figaro-season/stories/202310150123 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8B3173D0>, 'Connection to www.post-gazette.com timed out. (connect timeout=10)'))\n",
      "‚úÖ ƒê√£ l∆∞u to√†n b·ªô tin t·ª©c v√† n·ªôi dung b√†i vi·∫øt v√†o 'pittsburgh_opera_news.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "MAIN_URL = \"https://pittsburghopera.org/news-and-announcements\"\n",
    "\n",
    "def get_article_content(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # L·∫•y to√†n b·ªô vƒÉn b·∫£n t·ª´ b√†i vi·∫øt\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "        #return content[:2000] + \"...\" if len(content) > 2000 else content\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p link {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_pittsburgh_opera():\n",
    "    try:\n",
    "        res = requests.get(MAIN_URL)\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi truy c·∫≠p: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    items = soup.select(\"div.home-news-item\")\n",
    "\n",
    "    with open(\"pittsburgh_opera_news.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in items:\n",
    "            title_tag = item.select_one(\".title span\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"Kh√¥ng c√≥ ti√™u ƒë·ªÅ\"\n",
    "\n",
    "            link_tag = item.select_one(\"a\")\n",
    "            link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"\"\n",
    "\n",
    "            f.write(f\"üì∞ {title}\\n\")\n",
    "            f.write(f\"üîó {link}\\n\")\n",
    "\n",
    "            if link.startswith(\"http\"):\n",
    "                article = get_article_content(link)\n",
    "                if article:\n",
    "                    f.write(f\"üìÑ N·ªôi dung b√†i vi·∫øt:\\n{article}\\n\")\n",
    "\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    print(\"‚úÖ ƒê√£ l∆∞u to√†n b·ªô tin t·ª©c v√† n·ªôi dung b√†i vi·∫øt v√†o 'pittsburgh_opera_news.txt'\")\n",
    "\n",
    "crawl_pittsburgh_opera()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10fb9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/joinrenew/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/timed-tickets/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/plan-a-visit\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/things-to-do/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/things-to-do/view-our-exhibitions/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/things-to-do/explore-our-collections/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/things-to-do/learn-with-us/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/events/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/plan-a-visit/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/members-only-events/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/benefits/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/faq/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/reciprocal-privileges/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/donate/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/teen-membership/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/membership/access-for-all/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/carnegieconnectors/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/join-support/donate/corporate-gifts/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/great-event-spaces/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/carnegie-museums-in-oakland/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/carnegie-science-center/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/the-andy-warhol-museum/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/weddings-at-the-museums/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/contact-us/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/carnegie-magazine/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/tag/art/\n",
      "üï∏Ô∏è ƒêang crawl: https://carnegiemuseums.org/tag/science-nature/\n",
      "‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o 'carnegie_museums_data.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://carnegiemuseums.org/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def get_soup(url):\n",
    "    res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    res.raise_for_status()\n",
    "    return BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "def is_valid_link(href):\n",
    "    if not href or href.startswith(\"#\"):\n",
    "        return False\n",
    "    parsed = urlparse(href)\n",
    "    return not parsed.netloc or parsed.netloc == urlparse(BASE_URL).netloc\n",
    "\n",
    "def crawl_site(start_url, max_pages=20):\n",
    "    to_visit = [start_url]\n",
    "    result = []\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"üï∏Ô∏è ƒêang crawl: {url}\")\n",
    "            soup = get_soup(url)\n",
    "            visited.add(url)\n",
    "\n",
    "            title = soup.title.string.strip() if soup.title else \"Kh√¥ng c√≥ ti√™u ƒë·ªÅ\"\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            result.append((url, title, content[:1000] + \"...\" if len(content) > 1000 else content))\n",
    "\n",
    "            # L·∫•y th√™m link ƒë·ªÉ crawl ti·∫øp\n",
    "            links = soup.find_all(\"a\", href=True)\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if is_valid_link(href):\n",
    "                    full_url = urljoin(BASE_URL, href)\n",
    "                    if full_url not in visited and full_url not in to_visit:\n",
    "                        to_visit.append(full_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è L·ªói v·ªõi {url}: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_to_file(data, filename=\"carnegie_museums_data.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url, title, content in data:\n",
    "            f.write(f\"### {title}\\n\")\n",
    "            f.write(f\" {url}\\n\")\n",
    "            f.write(f\"# Content:\\n{content}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o '{filename}'\")\n",
    "\n",
    "# Ch·∫°y ch√≠nh\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_site(BASE_URL, max_pages=30)\n",
    "    save_to_file(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35370648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling: https://www.heinzhistorycenter.org/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/#page_title\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/#alert\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/history-center/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/sports-museum/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/fort-pitt/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/meadowcroft/\n",
      "üîç Crawling: https://my.heinzhistorycenter.org/orders/558/calendar?cart=true&eventId=64babd55351d6b414477592b&membershipIds=\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/join/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/give/make-a-donation/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/search/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/#menu\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/heinz-history-center/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/sports-museum/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/fort-pitt/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/meadowcroft/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/history-center/exhibits/pittsburghs-hidden-history/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/event/smithsonian-lecture-series-preserving-the-space-age/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/events/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/event/remake-learning-days-handmade-museum-05162025/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/event/from-calabria-to-carnegie-music-for-bread/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/event/vulture-the-private-life-of-an-unloved-bird/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/event/italian-genealogy-workshop/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/detre-library-archives/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/exhibits/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/families/\n",
      "üîç Crawling: https://shop.heinzhistorycenter.org/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/the-smithsonians-home-in-pittsburgh/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/explore/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/contact-us/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/accessibility/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/health-safety/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/work-with-us/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/sitemap/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/policies/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#224b4c444d624a474b4c584a4b51564d505b41474c5647500c4d5045\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#5b32353d341b333e3235213332282f342922383e352f3e297534293c\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#6c0a031e181c05181805020a032c040905021604051f18031e150f090218091e42031e0b\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/cdn-cgi/l/email-protection#e5888084818a9286978a83918c8b838aa58d808c8b9f8d8c96918a979c86808b918097cb8a9782\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/event-rentals/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/visit/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/whats-on/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/african-american-program/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/rauh-jewish-history-program-archives/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/italian-american-program/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/publications/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/research/contribute-to-our-collections/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/give/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/give/fundraising-events/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/give/commemorative-gifts/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/give/planned-giving/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/learn/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/learn/education/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/learn/scout-programs/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/learn/america-101/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/learn/virtual-tours/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/blog/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/learn/videos-and-podcasts/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/people/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/press-awards-honors/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/about/history-center-affiliates-program/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/groups/\n",
      "üîç Crawling: https://www.heinzhistorycenter.org/members/\n",
      "‚úÖ ƒê√£ l∆∞u 65 trang v√†o heinz_history_data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://www.heinzhistorycenter.org/\"\n",
    "visited = set()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc.endswith(\"heinzhistorycenter.org\")\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        res.raise_for_status()\n",
    "        return BeautifulSoup(res.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl(url, depth=0, max_depth=2):\n",
    "    if url in visited or depth > max_depth:\n",
    "        return []\n",
    "\n",
    "    visited.add(url)\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    data = []\n",
    "    print(f\"üîç Crawling: {url}\")\n",
    "\n",
    "    # L·∫•y n·ªôi dung vƒÉn b·∫£n\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    data.append(f\"# URL: {url}\\n## Content:\\n{content}\\n{'='*80}\")\n",
    "\n",
    "    # ƒê·ªá quy qua c√°c link n·ªôi b·ªô\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = urljoin(url, link[\"href\"])\n",
    "        if is_valid_url(href):\n",
    "            data.extend(crawl(href, depth + 1, max_depth))\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_file(data, filename=\"heinz_history_data.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(data))\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u {len(data)} trang v√†o {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = crawl(BASE_URL, max_depth=1)  # TƒÉng depth n·∫øu mu·ªën crawl s√¢u h∆°n\n",
    "    save_to_file(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a69c2c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Crawling: https://www.thefrickpittsburgh.org/\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/#top\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/stories\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/calendar\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/equity\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/membership\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/rentals\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/cafe\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/store\n",
      "üîç Crawling: https://tickets.thefrickpittsburgh.org/MembershipAndDonations.aspx?RegularOnly=True\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/plan-your-visit\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/plan-your-visit#visit\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/plan-your-visit#direction\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/Files/Admin/TheFrickPittsburghSiteMap2022.pdf?d=2022727165656\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/accessibility\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/tours\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/groupvisits\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/learn#education\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/plan-your-visit#plan\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/see-and-do\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p https://www.thefrickpittsburgh.org/see-and-do#box: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Read timed out. (read timeout=10)\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p https://www.thefrickpittsburgh.org/see-and-do#hap: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Read timed out. (read timeout=10)\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p https://www.thefrickpittsburgh.org/see-and-do#prog: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Read timed out. (read timeout=10)\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p https://www.thefrickpittsburgh.org/see-and-do#you: HTTPSConnectionPool(host='www.thefrickpittsburgh.org', port=443): Max retries exceeded with url: /see-and-do (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000011F8D6ED2A0>, 'Connection to www.thefrickpittsburgh.org timed out. (connect timeout=10)'))\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/see-and-do#planvisit\n",
      "‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p http://www.thefrickpittsburgh.org/learn: HTTPConnectionPool(host='www.thefrickpittsburgh.org', port=80): Read timed out. (read timeout=10)\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/learn#fam\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/learn#work\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/learn#adult\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/learn#summer\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/learn#out\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/virtual\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/exhibitions\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/exhibitions\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/past-exhibitions\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/virtualexhibitions\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Exhibition-Kara-Walker-Harpers-Pictorial-History-of-the-Civil-War-Annotated\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/collection\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/support\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/the-frick-societies\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/reciprocal-museums\n",
      "üîç Crawling: https://tickets.thefrickpittsburgh.org/Login.aspx\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/donate\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/LegacyPlanning\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/gifts-of-stock\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/Qualified-Charitable-Distributions\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/corporate-giving\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/eitc\n",
      "üîç Crawling: http://www.thefrickpittsburgh.org/oursupporters\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/tickets\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Signature-Clayton-Tour-Experience-Gilded-Not-Golden\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/carandcarriagemuseum\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Exhibition-Catching-Sunbeams\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Summer-Fridays-at-the-Frick\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Exhibition-The-Scandinavian-Home-Landscape-and-Lore\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/greenhouse-gardens\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/plan-your-visit\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Duquesne-SoundWalk-at-the-Frick\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Car-and-Carriage-Museum\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Grow-Pittsburgh-Seedling-Sale\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-The-Frick-Art-Museum\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Remake-Learning-Day-at-the-Frick\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/Event-Sip-Make-Kara-Walker\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/mediainquiries\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/employment\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/programmatic-collaborations\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/mission\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/strategicplan\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/boardoftrustees\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/board-portal\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/leadership\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/landacknowledgement\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/pressreleases\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/photography-policy\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/familyandlegacy\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/historichome\n",
      "üîç Crawling: https://www.thefrickpittsburgh.org/support-detail#contact\n",
      "‚úÖ ƒê√£ l∆∞u 72 trang v√†o Pittsburgh_frick_data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://www.thefrickpittsburgh.org/\"\n",
    "visited = set()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc.endswith(\"thefrickpittsburgh.org\")\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        return BeautifulSoup(res.text, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl(url, depth=0, max_depth=1):\n",
    "    if url in visited or depth > max_depth:\n",
    "        return []\n",
    "\n",
    "    visited.add(url)\n",
    "    soup = get_soup(url)\n",
    "    if not soup:\n",
    "        return []\n",
    "\n",
    "    print(f\"üîç Crawling: {url}\")\n",
    "    data = []\n",
    "\n",
    "    # L·∫•y n·ªôi dung t·ª´ c√°c th·∫ª ƒëo·∫°n vƒÉn\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    data.append(f\"# URL: {url}\\n## Content:\\n{content}\\n{'='*80}\")\n",
    "\n",
    "    # Crawl th√™m c√°c link n·ªôi b·ªô\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = urljoin(url, link[\"href\"])\n",
    "        if is_valid_url(href):\n",
    "            data.extend(crawl(href, depth + 1, max_depth))\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_file(data, filename=\"Pittsburgh_frick_data.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(data))\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u {len(data)} trang v√†o {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = crawl(BASE_URL, max_depth=1)\n",
    "    save_to_file(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e974b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u v√†o 'pittsburgh_museums_wiki.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_museums_in_Pittsburgh\"\n",
    "\n",
    "def get_page_content(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ truy c·∫≠p {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text(soup):\n",
    "    paragraphs = soup.select(\"p\")\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    return content\n",
    "\n",
    "def extract_tables(soup):\n",
    "    tables = soup.select(\"table.wikitable\")\n",
    "    extracted_tables = []\n",
    "\n",
    "    for table in tables:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        table_data = []\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all([\"th\", \"td\"])\n",
    "            cols_text = [col.get_text(strip=True) for col in cols]\n",
    "            if cols_text:\n",
    "                table_data.append(cols_text)\n",
    "\n",
    "        extracted_tables.append(table_data)\n",
    "    return extracted_tables\n",
    "\n",
    "def save_to_file(text, tables, filename=\"pittsburgh_museums_wiki.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"üìÑ N·ªôi dung vƒÉn b·∫£n:\\n\")\n",
    "        f.write(text + \"\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        for i, table in enumerate(tables):\n",
    "            f.write(f\"üìä B·∫£ng {i+1}:\\n\")\n",
    "            for row in table:\n",
    "                f.write(\" | \".join(row) + \"\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u v√†o '{filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    soup = get_page_content(URL)\n",
    "    if soup:\n",
    "        text = extract_text(soup)\n",
    "        tables = extract_tables(soup)\n",
    "        save_to_file(text, tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156a61b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Ghi v√†o file duy nh·∫•t\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ ƒê√£ ho√†n t·∫•t ghi to√†n b·ªô header v√† paragraph v√†o \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m link \u001b[38;5;241m=\u001b[39m urljoin(url, a_tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(link) \u001b[38;5;129;01mand\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m link \u001b[38;5;241m=\u001b[39m urljoin(url, a_tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(link) \u001b[38;5;129;01mand\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: crawl_page at line 52 (79 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     50\u001b[0m link \u001b[38;5;241m=\u001b[39m urljoin(url, a_tag[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_url(link) \u001b[38;5;129;01mand\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mcrawl_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mcrawl_page\u001b[1;34m(url, file)\u001b[0m\n\u001b[0;32m     21\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1368\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:317\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:278\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 278\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\dongh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "BASE_URL = \"https://bananasplitfest.com/\"\n",
    "DOMAIN = urlparse(BASE_URL).netloc\n",
    "visited = set()\n",
    "\n",
    "output_file = \"Banana Split Fest.txt\"\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return (parsed.scheme in [\"http\", \"https\"] and\n",
    "            (parsed.netloc == \"\" or DOMAIN in parsed.netloc))\n",
    "\n",
    "def crawl_page(url, file):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi truy c·∫≠p {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    body = soup.body\n",
    "    if not body:\n",
    "        return\n",
    "\n",
    "    file.write(f\"\\n{'='*80}\\nüìå URL: {url}\\n\\n\")\n",
    "    \n",
    "    for element in body.descendants:\n",
    "        if element.name in [f\"h{i}\" for i in range(1, 7)]:\n",
    "            text = element.get_text(strip=True)\n",
    "            if text:\n",
    "                file.write(f\"## {text}\\n\")\n",
    "        elif element.name == \"p\":\n",
    "            text = element.get_text(strip=True)\n",
    "            if text:\n",
    "                file.write(f\"[CONTENT] {text}\\n\")\n",
    "\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        link = urljoin(url, a_tag['href'])\n",
    "        if is_valid_url(link) and link not in visited:\n",
    "            crawl_page(link, file)\n",
    "\n",
    "# Ghi v√†o file duy nh·∫•t\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    crawl_page(BASE_URL, f)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ ho√†n t·∫•t ghi to√†n b·ªô header v√† paragraph v√†o {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
