{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50814017",
   "metadata": {},
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517aeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_wikipedia(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'id': 'bodyContent'})\n",
    "\n",
    "    if not content_div:\n",
    "        print(\"Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh.\")\n",
    "        return\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(allowed_tags):\n",
    "            # Xo√° ch√∫ th√≠ch trong th·∫ª <p>\n",
    "            if tag.name == 'p':\n",
    "                for sup in tag.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_wikipedia(\"https://en.wikipedia.org/wiki/Pittsburgh\", \"pittsburgh.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dda932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://en.wikipedia.org/wiki/Pittsburgh v√†o 'pittsburgh.txt'\n",
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://en.wikipedia.org/wiki/History_of_Pittsburgh v√†o 'history_of_pittsburgh.txt'\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://en.wikipedia.org/wiki/Pittsburgh\"\n",
    "url2 = \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\"\n",
    "output_file1 = \"pittsburgh.txt\"\n",
    "output_file2 = \"history_of_pittsburgh.txt\"\n",
    "\n",
    "crawl_wikipedia(url1, output_file1)\n",
    "crawl_wikipedia(url2, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780bdc",
   "metadata": {},
   "source": [
    "britannica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_britannica(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"L·ªói khi truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # C√°c th·∫ª mu·ªën gi·ªØ l·∫°i theo th·ª© t·ª± xu·∫•t hi·ªán\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_britannica(\"https://www.britannica.com/place/Pittsburgh\", \"pittsburgh_britannica.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718a27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://www.britannica.com/place/Pittsburgh v√†o 'pittsburgh_britannica.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.britannica.com/place/Pittsburgh\"\n",
    "output_file = \"pittsburgh_britannica.txt\"\n",
    "crawl_britannica(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c3b5d",
   "metadata": {},
   "source": [
    "dynamic web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def crawl_dynamic_site(url, output_file):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # ch·∫°y ·∫©n\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    service = Service(r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # ch·ªù trang load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ {url} v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "# crawl_dynamic_site(\"https://www.visitpittsburgh.com/\", \"visitpittsburgh.txt\", driver_path=\"/path/to/chromedriver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4d6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u n·ªôi dung t·ª´ https://www.visitpittsburgh.com/ v√†o 'Pittsburgh webpage.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.visitpittsburgh.com/\"\n",
    "output_file = \"Pittsburgh webpage.txt\"\n",
    "crawl_dynamic_site(url=url, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "757f6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9622_amusement_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9626_payroll_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9623_isp_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9624_local_services_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9625_parking_tax_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 9627_uf_regulations.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: change-in-business-status-form-04.2025.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6492_2636_10_taxpayers_bill_of_rights_4-26-2018.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 16958_2022_tax_rate_by_tax_type.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 16957_2022_tax_due_date_calendar_.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8271_facility_usage_fee_information_for_performers_and_contracting_parties.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: firesale.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8398_payroll_expense_tax__et__allocation_schedule_form.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6825_payroll_expense_tax_allocation_schedule_for_professional_organization_form_instructions8.15.19.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8397_local_services_tax_ls-1_allocation_schedule_form.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6822_local_service_tax_allocation_schedule_for_professional_employer_organization_form_instructions8.15.19.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 8403_general_contractor_detail_report.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 7065_general_contractor_detail_report_instructions.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: ls-tax-exemption-certificate-2025.pdf\n",
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: 6819__lst_refund_form.pdf\n",
      "\n",
      "üìÑ ƒê√£ ghi to√†n b·ªô n·ªôi dung v√†o 'pittsburgh_tax.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def crawl_and_extract(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'class': 'main-container clearfix'})\n",
    "    if not content_div:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y v√πng n·ªôi dung ch√≠nh.\")\n",
    "        return\n",
    "\n",
    "    tags = ['h1', 'h2', 'h3', 'h4', 'p', 'a']\n",
    "    found_regulation = False\n",
    "    base_path = \"pdf_temp\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(tags):\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                text = tag.get_text(strip=True)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if tag.name == 'h1':\n",
    "                    f.write(f\"# {text}\\n\\n\")\n",
    "                elif tag.name == 'h2':\n",
    "                    f.write(f\"## {text}\\n\\n\")\n",
    "                elif tag.name == 'h3':\n",
    "                    f.write(f\"### {text}\\n\\n\")\n",
    "                elif tag.name == 'h4':\n",
    "                    f.write(f\"#### {text}\\n\\n\")\n",
    "                    if 'regulations' in text.lower():\n",
    "                        found_regulation = True\n",
    "                    else:\n",
    "                        found_regulation = False  # reset khi g·∫∑p h4 kh√°c\n",
    "\n",
    "            elif tag.name == 'p':\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    f.write(text + '\\n\\n')\n",
    "\n",
    "            elif tag.name == 'a' and found_regulation:\n",
    "                href = tag.get('href', '')\n",
    "                if href.endswith('.pdf'):\n",
    "                    pdf_url = urljoin(url, href)\n",
    "                    filename = os.path.basename(href)\n",
    "                    local_pdf = os.path.join(base_path, filename)\n",
    "\n",
    "                    try:\n",
    "                        pdf_res = requests.get(pdf_url, headers=headers)\n",
    "                        with open(local_pdf, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_res.content)\n",
    "                        f.write(f\"üìé N·ªôi dung t·ª´ file PDF: {filename}\\n\\n\")\n",
    "\n",
    "                        # Tr√≠ch n·ªôi dung PDF\n",
    "                        doc = fitz.open(local_pdf)\n",
    "                        for page in doc:\n",
    "                            text = page.get_text().strip()\n",
    "                            if text:\n",
    "                                f.write(text + '\\n\\n')\n",
    "                        doc.close()\n",
    "                        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t PDF: {filename}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"[L·ªói khi t·∫£i ho·∫∑c ƒë·ªçc PDF {filename}: {e}]\\n\\n\")\n",
    "\n",
    "    print(f\"\\nüìÑ ƒê√£ ghi to√†n b·ªô n·ªôi dung v√†o '{output_file}'\")\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng:\n",
    "crawl_and_extract(\n",
    "    \"https://www.pittsburghpa.gov/City-Government/Finances-Budget/Taxes/Tax-Forms\",\n",
    "    \"pittsburgh_tax.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e509a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n v√†o '2024 Operating Budget.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # ·∫®n c·∫£nh b√°o SSL\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url: str, output_txt: str = \"output.txt\"):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, verify=False)  # ‚ö†Ô∏è B·ªè qua SSL verify\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        doc = fitz.open(\"temp.pdf\")\n",
    "        all_text = []\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            # Gi·ªØ nguy√™n xu·ªëng d√≤ng, nh∆∞ng lo·∫°i kho·∫£ng tr·∫Øng th·ª´a\n",
    "            cleaned_lines = [line.strip() for line in page_text.splitlines() if line.strip()]\n",
    "            all_text.extend(cleaned_lines)\n",
    "            all_text.append(\"\")  # Th√™m d√≤ng tr·ªëng gi·ªØa c√°c trang (t√πy ch·ªçn)\n",
    "\n",
    "        # Ghi ra file, m·ªói d√≤ng l√† 1 d√≤ng vƒÉn b·∫£n r√µ r√†ng\n",
    "        with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_text))\n",
    "\n",
    "        print(f\"‚úÖ ƒê√£ tr√≠ch xu·∫•t vƒÉn b·∫£n v√†o '{output_txt}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói: {e}\")\n",
    "\n",
    "extract_text_from_pdf_url(\n",
    "     \"https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf\",\n",
    "     \"2024 Operating Budget.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb5fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawled: https://www.cmu.edu/about/\n",
      "‚úÖ Crawled: https://www.cmu.edu/academics/interdisciplinary-programs.html\n",
      "‚úÖ Crawled: https://www.library.cmu.edu/\n",
      "‚úÖ Crawled: https://www.cmu.edu/academics/learning-for-a-lifetime.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/admission/student-community-blog\n",
      "‚úÖ Crawled: https://www.cmu.edu/graduate/prospective/index.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/leadership/\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/mission.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/history.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/traditions.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/inclusive-excellence/\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/pittsburgh.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/rankings.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/about/awards.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/visit//visitor-information\n",
      "‚úÖ Crawled: https://www.cmu.edu/research/centers-and-institutes.html\n",
      "‚úÖ Crawled: https://www.cmu.edu/student-experience/index.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "urls = [\n",
    "    \"https://www.cmu.edu/about/\",\n",
    "    \"https://www.cmu.edu/academics/interdisciplinary-programs.html\",\n",
    "    \"https://www.library.cmu.edu/\",\n",
    "    \"https://www.cmu.edu/academics/learning-for-a-lifetime.html\",\n",
    "    \"https://www.cmu.edu/admission/student-community-blog\",\n",
    "    \"https://www.cmu.edu/graduate/prospective/index.html\",\n",
    "    \"https://www.cmu.edu/leadership/\",\n",
    "    \"https://www.cmu.edu/about/mission.html\",\n",
    "    \"https://www.cmu.edu/about/history.html\",\n",
    "    \"https://www.cmu.edu/about/traditions.html\",\n",
    "    \"https://www.cmu.edu/inclusive-excellence/\",\n",
    "    \"https://www.cmu.edu/about/pittsburgh.html\",\n",
    "    \"https://www.cmu.edu/about/rankings.html\",\n",
    "    \"https://www.cmu.edu/about/awards.html\",\n",
    "    \"https://www.cmu.edu/visit//visitor-information\",\n",
    "    \"https://www.cmu.edu/research/centers-and-institutes.html\",\n",
    "    \"https://www.cmu.edu/student-experience/index.html\",\n",
    "]\n",
    "\n",
    "def extract_header(url):\n",
    "    # L·∫•y ph·∫ßn sau c√πng c·ªßa URL ƒë·ªÉ l√†m ti√™u ƒë·ªÅ\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip(\"/\").split(\"/\")[-1]\n",
    "    if not path or path == \"index.html\":\n",
    "        path = parsed.path.strip(\"/\").split(\"/\")[-2] if len(parsed.path.strip(\"/\").split(\"/\")) > 1 else \"home\"\n",
    "    path = re.sub(r'\\.html$', '', path)\n",
    "    return \"#\" + path.lower().replace(\" \", \"-\")\n",
    "\n",
    "def clean_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    main = soup.find(\"main\") or soup.body\n",
    "\n",
    "    if main is None:\n",
    "        return \"\"\n",
    "\n",
    "    for tag in main.find_all([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = main.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def crawl_and_write(urls, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                print(f\"‚úÖ Crawled: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói truy c·∫≠p {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            header = extract_header(url)\n",
    "            text = clean_text(response.text)\n",
    "\n",
    "            if text:\n",
    "                f.write(header + \"\\n\")\n",
    "                f.write(text + \"\\n\\n\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung ·ªü {url}\")\n",
    "\n",
    "# G·ªçi h√†m crawl\n",
    "crawl_and_write(urls, \"cmu_about.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027b9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang crawl trang 1...\n",
      "üîç ƒêang crawl trang 2...\n",
      "üîç ƒêang crawl trang 3...\n",
      "üîç ƒêang crawl trang 4...\n",
      "üîç ƒêang crawl trang 5...\n",
      "üîç ƒêang crawl trang 6...\n",
      "üîç ƒêang crawl trang 7...\n",
      "üîç ƒêang crawl trang 8...\n",
      "üîç ƒêang crawl trang 9...\n",
      "üîç ƒêang crawl trang 10...\n",
      "üîç ƒêang crawl trang 11...\n",
      "üîç ƒêang crawl trang 12...\n",
      "üîç ƒêang crawl trang 13...\n",
      "üîç ƒêang crawl trang 14...\n",
      "üîç ƒêang crawl trang 15...\n",
      "üîç ƒêang crawl trang 16...\n",
      "üîç ƒêang crawl trang 17...\n",
      "üîç ƒêang crawl trang 18...\n",
      "üîç ƒêang crawl trang 19...\n",
      "üîç ƒêang crawl trang 20...\n",
      "üîç ƒêang crawl trang 21...\n",
      "üîç ƒêang crawl trang 22...\n",
      "üîç ƒêang crawl trang 23...\n",
      "üîç ƒêang crawl trang 24...\n",
      "üîç ƒêang crawl trang 25...\n",
      "üîç ƒêang crawl trang 26...\n",
      "üîç ƒêang crawl trang 27...\n",
      "üîç ƒêang crawl trang 28...\n",
      "üîç ƒêang crawl trang 29...\n",
      "üîç ƒêang crawl trang 30...\n",
      "üîç ƒêang crawl trang 31...\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o 'Downtown Pittsburgh events calendar.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_all_events(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 32):  # t·ª´ 1 ƒë·∫øn 31\n",
    "            url = f\"https://downtownpittsburgh.com/events/?n=5&d={page}&y=2025\"\n",
    "            print(f\"üîç ƒêang crawl trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            h1_tags = soup.find_all('h1')\n",
    "\n",
    "            if not h1_tags:\n",
    "                print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán tr√™n trang {page}\")\n",
    "                continue\n",
    "\n",
    "            for h1 in h1_tags:\n",
    "                link_tag = h1.find('a')\n",
    "                if not link_tag:\n",
    "                    continue\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                href = link_tag.get('href', '')\n",
    "                full_url = 'https://downtownpittsburgh.com' + href\n",
    "\n",
    "                sibling = h1.next_sibling\n",
    "                date_text = None\n",
    "                description_parts = []\n",
    "\n",
    "                while sibling:\n",
    "                    if not hasattr(sibling, 'get_text'):\n",
    "                        sibling = sibling.next_sibling\n",
    "                        continue\n",
    "\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text:\n",
    "                        if not date_text and ('am' in text.lower() or 'pm' in text.lower() or '-' in text):\n",
    "                            date_text = text\n",
    "                        elif text == \"READ MORE\":\n",
    "                            break\n",
    "                        else:\n",
    "                            description_parts.append(text)\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "                if not date_text:\n",
    "                    date_text = \"No date\"\n",
    "                description = \" \".join(description_parts).strip() if description_parts else \"No description\"\n",
    "\n",
    "                f.write(f\"## [{title}]({full_url})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date_text}\\n\")\n",
    "                f.write(f\"**Description**: {description}\\n\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o '{output_file}'\")\n",
    "\n",
    "\n",
    "crawl_all_events(\"Downtown Pittsburgh events calendar.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "477a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ƒêang x·ª≠ l√Ω trang 1...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 2...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 3...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 4...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 5...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 6...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 7...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 8...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 9...\n",
      "üîé ƒêang x·ª≠ l√Ω trang 10...\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u s·ª± ki·ªán v√†o 'citypaper_events.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_citypaper_detailed(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 11):\n",
    "            url = f\"https://www.pghcitypaper.com/pittsburgh/EventSearch?page={page}&v=d\"\n",
    "            print(f\"üîé ƒêang x·ª≠ l√Ω trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå L·ªói truy c·∫≠p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            events = soup.find_all('p', class_='fdn-teaser-headline')\n",
    "\n",
    "            for headline in events:\n",
    "                # L·∫•y ti√™u ƒë·ªÅ v√† link\n",
    "                a_tag = headline.find('a', href=True)\n",
    "                if not a_tag:\n",
    "                    continue\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                link = a_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://www.pghcitypaper.com' + link\n",
    "\n",
    "                # T√¨m c√°c ph·∫ßn c√≤n l·∫°i th√¥ng qua cha c·ªßa headline\n",
    "                parent = headline.find_parent()\n",
    "                date_tag = parent.find_next('p', class_='fdn-teaser-subheadline')\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"No date\"\n",
    "\n",
    "                loc_tag = parent.find_next('a', class_='fdn-event-teaser-location-link')\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else \"No location\"\n",
    "\n",
    "                addr_tag = parent.find_next('p', class_='fdn-inline-split-list')\n",
    "                address = addr_tag.get_text(strip=True) if addr_tag else \"No address\"\n",
    "\n",
    "                desc_tag = parent.find_next('div', class_='fdn-teaser-description')\n",
    "                desc = desc_tag.get_text(strip=True) if desc_tag else \"No description\"\n",
    "\n",
    "                # Ghi ra file\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date}\\n\")\n",
    "                f.write(f\"**Location**: {location}, {address}\\n\")\n",
    "                f.write(f\"**Description**: {desc}\\n\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ ƒê√£ l∆∞u s·ª± ki·ªán v√†o '{output_file}'\")\n",
    "\n",
    "# G·ªçi h√†m:\n",
    "crawl_citypaper_detailed(\"citypaper_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6386f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250401\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250402\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250403\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250404\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250405\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250406\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250407\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250408\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250409\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250410\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250411\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250412\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250413\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250414\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250415\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250416\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250417\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250418\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250419\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250420\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250421\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250422\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250423\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250424\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250425\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250426\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250427\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250428\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250429\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250430\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250501\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250502\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250503\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250504\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250505\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250506\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250507\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250508\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250509\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250510\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250511\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250512\n",
      "üîé ƒêang x·ª≠ l√Ω: https://events.cmu.edu/day/date/20250513\n",
      "\n",
      "‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o 'cmu_events.txt'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def crawl_cmu_events_selenium(start_date, end_date, output_file):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # kh√¥ng m·ªü c·ª≠a s·ªï tr√¨nh duy·ªát\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime('%Y%m%d')\n",
    "            url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "            print(f\"üîé ƒêang x·ª≠ l√Ω: {url}\")\n",
    "\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # ƒë·ª£i trang load JS xong\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            events = soup.find_all('div', class_='lw_cal_event_info')\n",
    "\n",
    "            if not events:\n",
    "                current_date += timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            for event in events:\n",
    "                title_tag = event.find('div', class_='lw_events_title')\n",
    "                title = title_tag.a.get_text(strip=True) if title_tag and title_tag.a else \"Kh√¥ng c√≥ ti√™u ƒë·ªÅ\"\n",
    "                link = title_tag.a['href'] if title_tag and title_tag.a else \"#\"\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://events.cmu.edu' + link\n",
    "\n",
    "                location = event.find('div', class_='lw_events_location')\n",
    "                time_tag = event.find('div', class_='lw_events_time')\n",
    "                summary = event.find('div', class_='lw_events_summary')\n",
    "\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date**: {current_date.strftime('%Y-%m-%d')}\\n\")\n",
    "                f.write(f\"**Time**: {time_tag.get_text(strip=True) if time_tag else 'Kh√¥ng r√µ'}\\n\")\n",
    "                f.write(f\"**Location**: {location.get_text(strip=True) if location else 'Kh√¥ng r√µ'}\\n\")\n",
    "                f.write(f\"**Summary**: {summary.get_text(strip=True) if summary else 'Kh√¥ng c√≥'}\\n\\n\")\n",
    "\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\n‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ s·ª± ki·ªán v√†o '{output_file}'\")\n",
    "\n",
    "# G·ªçi h√†m\n",
    "start = datetime(2025, 4, 1)\n",
    "end = datetime(2025, 5, 13)\n",
    "crawl_cmu_events_selenium(start, end, \"cmu_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a5f43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o pittsburgh_events.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† kho·∫£ng tr·∫Øng th·ª´a\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "def crawl_events():\n",
    "    url = \"https://www.visitpittsburgh.com/events-festivals/?page=11\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # G·ª≠i y√™u c·∫ßu GET ƒë·∫øn trang web\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Ph√¢n t√≠ch HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # T√¨m t·∫•t c·∫£ c√°c s·ª± ki·ªán\n",
    "        events = soup.find_all('div', class_='card card--common card--listing')\n",
    "        \n",
    "        # M·ªü file ƒë·ªÉ l∆∞u d·ªØ li·ªáu\n",
    "        with open('pittsburgh_events.txt', 'w', encoding='utf-8') as f:\n",
    "            for event in events:\n",
    "                # L·∫•y ti√™u ƒë·ªÅ\n",
    "                title_elem = event.find('a', class_='card_heading')\n",
    "                title = clean_text(title_elem.text) if title_elem else 'N/A'\n",
    "                \n",
    "                # L·∫•y ng√†y\n",
    "                date_elem = event.find('p', class_='date-heading card_date-heading')\n",
    "                date = clean_text(date_elem.text) if date_elem else 'N/A'\n",
    "                \n",
    "                # L·∫•y ƒë·ªãa ch·ªâ\n",
    "                address_elem = event.find('div', class_='card____address')\n",
    "                address = clean_text(address_elem.text) if address_elem else 'N/A'\n",
    "                \n",
    "                # L·∫•y s·ªë ƒëi·ªán tho·∫°i\n",
    "                phone_elem = event.find('div', class_='card____phone')\n",
    "                phone = clean_text(phone_elem.text) if phone_elem else 'N/A'\n",
    "                \n",
    "                # Ghi v√†o file\n",
    "                f.write(f\"Title: {title}\\n\")\n",
    "                f.write(f\"Date: {date}\\n\")\n",
    "                f.write(f\"Address: {address}\\n\")\n",
    "                f.write(f\"Phone: {phone}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                \n",
    "        print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o pittsburgh_events.txt\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"L·ªói khi truy c·∫≠p trang web: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_events()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
