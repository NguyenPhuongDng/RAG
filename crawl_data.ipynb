{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50814017",
   "metadata": {},
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517aeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_wikipedia(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Lá»—i khi truy cáº­p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'id': 'bodyContent'})\n",
    "\n",
    "    if not content_div:\n",
    "        print(\"KhÃ´ng tÃ¬m tháº¥y ná»™i dung chÃ­nh.\")\n",
    "        return\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(allowed_tags):\n",
    "            # XoÃ¡ chÃº thÃ­ch trong tháº» <p>\n",
    "            if tag.name == 'p':\n",
    "                for sup in tag.find_all('sup'):\n",
    "                    sup.decompose()\n",
    "\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u ná»™i dung tá»« {url} vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "# crawl_wikipedia(\"https://en.wikipedia.org/wiki/Pittsburgh\", \"pittsburgh.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dda932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://en.wikipedia.org/wiki/Pittsburgh vÃ o 'pittsburgh.txt'\n",
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://en.wikipedia.org/wiki/History_of_Pittsburgh vÃ o 'history_of_pittsburgh.txt'\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://en.wikipedia.org/wiki/Pittsburgh\"\n",
    "url2 = \"https://en.wikipedia.org/wiki/History_of_Pittsburgh\"\n",
    "output_file1 = \"pittsburgh.txt\"\n",
    "output_file2 = \"history_of_pittsburgh.txt\"\n",
    "\n",
    "crawl_wikipedia(url1, output_file1)\n",
    "crawl_wikipedia(url2, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5780bdc",
   "metadata": {},
   "source": [
    "britannica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cddd4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_britannica(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Lá»—i khi truy cáº­p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # CÃ¡c tháº» muá»‘n giá»¯ láº¡i theo thá»© tá»± xuáº¥t hiá»‡n\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u ná»™i dung tá»« {url} vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "# crawl_britannica(\"https://www.britannica.com/place/Pittsburgh\", \"pittsburgh_britannica.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718a27a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://www.britannica.com/place/Pittsburgh vÃ o 'pittsburgh_britannica.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.britannica.com/place/Pittsburgh\"\n",
    "output_file = \"pittsburgh_britannica.txt\"\n",
    "crawl_britannica(url, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c3b5d",
   "metadata": {},
   "source": [
    "dynamic web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def crawl_dynamic_site(url, output_file):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # cháº¡y áº©n\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    service = Service(r\"C:\\Users\\dongh\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # chá» trang load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    allowed_tags = ['h1', 'h2', 'h3', 'p']\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in soup.find_all(allowed_tags):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if tag.name == 'h1':\n",
    "                f.write(f\"# {text}\\n\\n\")\n",
    "            elif tag.name == 'h2':\n",
    "                f.write(f\"## {text}\\n\\n\")\n",
    "            elif tag.name == 'h3':\n",
    "                f.write(f\"### {text}\\n\\n\")\n",
    "            else:\n",
    "                f.write(text + '\\n\\n')\n",
    "\n",
    "    print(f\"âœ… ÄÃ£ lÆ°u ná»™i dung tá»« {url} vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "# crawl_dynamic_site(\"https://www.visitpittsburgh.com/\", \"visitpittsburgh.txt\", driver_path=\"/path/to/chromedriver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb4d6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u ná»™i dung tá»« https://www.visitpittsburgh.com/ vÃ o 'Pittsburgh webpage.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.visitpittsburgh.com/\"\n",
    "output_file = \"Pittsburgh webpage.txt\"\n",
    "crawl_dynamic_site(url=url, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "757f6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9622_amusement_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9626_payroll_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9623_isp_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9624_local_services_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9625_parking_tax_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 9627_uf_regulations.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: change-in-business-status-form-04.2025.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6492_2636_10_taxpayers_bill_of_rights_4-26-2018.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 16958_2022_tax_rate_by_tax_type.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 16957_2022_tax_due_date_calendar_.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8271_facility_usage_fee_information_for_performers_and_contracting_parties.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: firesale.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8398_payroll_expense_tax__et__allocation_schedule_form.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6825_payroll_expense_tax_allocation_schedule_for_professional_organization_form_instructions8.15.19.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8397_local_services_tax_ls-1_allocation_schedule_form.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6822_local_service_tax_allocation_schedule_for_professional_employer_organization_form_instructions8.15.19.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 8403_general_contractor_detail_report.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 7065_general_contractor_detail_report_instructions.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: ls-tax-exemption-certificate-2025.pdf\n",
      "âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: 6819__lst_refund_form.pdf\n",
      "\n",
      "ğŸ“„ ÄÃ£ ghi toÃ n bá»™ ná»™i dung vÃ o 'pittsburgh_tax.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def crawl_and_extract(url, output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"âŒ Lá»—i truy cáº­p {url}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content_div = soup.find('div', {'class': 'main-container clearfix'})\n",
    "    if not content_div:\n",
    "        print(\"âŒ KhÃ´ng tÃ¬m tháº¥y vÃ¹ng ná»™i dung chÃ­nh.\")\n",
    "        return\n",
    "\n",
    "    tags = ['h1', 'h2', 'h3', 'h4', 'p', 'a']\n",
    "    found_regulation = False\n",
    "    base_path = \"pdf_temp\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for tag in content_div.find_all(tags):\n",
    "            if tag.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                text = tag.get_text(strip=True)\n",
    "                if not text:\n",
    "                    continue\n",
    "                if tag.name == 'h1':\n",
    "                    f.write(f\"# {text}\\n\\n\")\n",
    "                elif tag.name == 'h2':\n",
    "                    f.write(f\"## {text}\\n\\n\")\n",
    "                elif tag.name == 'h3':\n",
    "                    f.write(f\"### {text}\\n\\n\")\n",
    "                elif tag.name == 'h4':\n",
    "                    f.write(f\"#### {text}\\n\\n\")\n",
    "                    if 'regulations' in text.lower():\n",
    "                        found_regulation = True\n",
    "                    else:\n",
    "                        found_regulation = False  # reset khi gáº·p h4 khÃ¡c\n",
    "\n",
    "            elif tag.name == 'p':\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    f.write(text + '\\n\\n')\n",
    "\n",
    "            elif tag.name == 'a' and found_regulation:\n",
    "                href = tag.get('href', '')\n",
    "                if href.endswith('.pdf'):\n",
    "                    pdf_url = urljoin(url, href)\n",
    "                    filename = os.path.basename(href)\n",
    "                    local_pdf = os.path.join(base_path, filename)\n",
    "\n",
    "                    try:\n",
    "                        pdf_res = requests.get(pdf_url, headers=headers)\n",
    "                        with open(local_pdf, 'wb') as pdf_file:\n",
    "                            pdf_file.write(pdf_res.content)\n",
    "                        f.write(f\"ğŸ“ Ná»™i dung tá»« file PDF: {filename}\\n\\n\")\n",
    "\n",
    "                        # TrÃ­ch ná»™i dung PDF\n",
    "                        doc = fitz.open(local_pdf)\n",
    "                        for page in doc:\n",
    "                            text = page.get_text().strip()\n",
    "                            if text:\n",
    "                                f.write(text + '\\n\\n')\n",
    "                        doc.close()\n",
    "                        print(f\"âœ… ÄÃ£ trÃ­ch xuáº¥t PDF: {filename}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"[Lá»—i khi táº£i hoáº·c Ä‘á»c PDF {filename}: {e}]\\n\\n\")\n",
    "\n",
    "    print(f\"\\nğŸ“„ ÄÃ£ ghi toÃ n bá»™ ná»™i dung vÃ o '{output_file}'\")\n",
    "\n",
    "# VÃ­ dá»¥ sá»­ dá»¥ng:\n",
    "crawl_and_extract(\n",
    "    \"https://www.pittsburghpa.gov/City-Government/Finances-Budget/Taxes/Tax-Forms\",\n",
    "    \"pittsburgh_tax.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e509a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ trÃ­ch xuáº¥t vÄƒn báº£n vÃ o '2024 Operating Budget.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # áº¨n cáº£nh bÃ¡o SSL\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url: str, output_txt: str = \"output.txt\"):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, verify=False)  # âš ï¸ Bá» qua SSL verify\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        doc = fitz.open(\"temp.pdf\")\n",
    "        all_text = []\n",
    "\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            # Giá»¯ nguyÃªn xuá»‘ng dÃ²ng, nhÆ°ng loáº¡i khoáº£ng tráº¯ng thá»«a\n",
    "            cleaned_lines = [line.strip() for line in page_text.splitlines() if line.strip()]\n",
    "            all_text.extend(cleaned_lines)\n",
    "            all_text.append(\"\")  # ThÃªm dÃ²ng trá»‘ng giá»¯a cÃ¡c trang (tÃ¹y chá»n)\n",
    "\n",
    "        # Ghi ra file, má»—i dÃ²ng lÃ  1 dÃ²ng vÄƒn báº£n rÃµ rÃ ng\n",
    "        with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_text))\n",
    "\n",
    "        print(f\"âœ… ÄÃ£ trÃ­ch xuáº¥t vÄƒn báº£n vÃ o '{output_txt}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i: {e}\")\n",
    "\n",
    "extract_text_from_pdf_url(\n",
    "     \"https://apps.pittsburghpa.gov/redtail/images/23255_2024_Operating_Budget.pdf\",\n",
    "     \"2024 Operating Budget.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb5fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: https://www.cmu.edu/about/\n",
      "âœ… Crawled: https://www.cmu.edu/academics/interdisciplinary-programs.html\n",
      "âœ… Crawled: https://www.library.cmu.edu/\n",
      "âœ… Crawled: https://www.cmu.edu/academics/learning-for-a-lifetime.html\n",
      "âœ… Crawled: https://www.cmu.edu/admission/student-community-blog\n",
      "âœ… Crawled: https://www.cmu.edu/graduate/prospective/index.html\n",
      "âœ… Crawled: https://www.cmu.edu/leadership/\n",
      "âœ… Crawled: https://www.cmu.edu/about/mission.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/history.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/traditions.html\n",
      "âœ… Crawled: https://www.cmu.edu/inclusive-excellence/\n",
      "âœ… Crawled: https://www.cmu.edu/about/pittsburgh.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/rankings.html\n",
      "âœ… Crawled: https://www.cmu.edu/about/awards.html\n",
      "âœ… Crawled: https://www.cmu.edu/visit//visitor-information\n",
      "âœ… Crawled: https://www.cmu.edu/research/centers-and-institutes.html\n",
      "âœ… Crawled: https://www.cmu.edu/student-experience/index.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "urls = [\n",
    "    \"https://www.cmu.edu/about/\",\n",
    "    \"https://www.cmu.edu/academics/interdisciplinary-programs.html\",\n",
    "    \"https://www.library.cmu.edu/\",\n",
    "    \"https://www.cmu.edu/academics/learning-for-a-lifetime.html\",\n",
    "    \"https://www.cmu.edu/admission/student-community-blog\",\n",
    "    \"https://www.cmu.edu/graduate/prospective/index.html\",\n",
    "    \"https://www.cmu.edu/leadership/\",\n",
    "    \"https://www.cmu.edu/about/mission.html\",\n",
    "    \"https://www.cmu.edu/about/history.html\",\n",
    "    \"https://www.cmu.edu/about/traditions.html\",\n",
    "    \"https://www.cmu.edu/inclusive-excellence/\",\n",
    "    \"https://www.cmu.edu/about/pittsburgh.html\",\n",
    "    \"https://www.cmu.edu/about/rankings.html\",\n",
    "    \"https://www.cmu.edu/about/awards.html\",\n",
    "    \"https://www.cmu.edu/visit//visitor-information\",\n",
    "    \"https://www.cmu.edu/research/centers-and-institutes.html\",\n",
    "    \"https://www.cmu.edu/student-experience/index.html\",\n",
    "]\n",
    "\n",
    "def extract_header(url):\n",
    "    # Láº¥y pháº§n sau cÃ¹ng cá»§a URL Ä‘á»ƒ lÃ m tiÃªu Ä‘á»\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip(\"/\").split(\"/\")[-1]\n",
    "    if not path or path == \"index.html\":\n",
    "        path = parsed.path.strip(\"/\").split(\"/\")[-2] if len(parsed.path.strip(\"/\").split(\"/\")) > 1 else \"home\"\n",
    "    path = re.sub(r'\\.html$', '', path)\n",
    "    return \"#\" + path.lower().replace(\" \", \"-\")\n",
    "\n",
    "def clean_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    main = soup.find(\"main\") or soup.body\n",
    "\n",
    "    if main is None:\n",
    "        return \"\"\n",
    "\n",
    "    for tag in main.find_all([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = main.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def crawl_and_write(urls, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for url in urls:\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                print(f\"âœ… Crawled: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Lá»—i truy cáº­p {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            header = extract_header(url)\n",
    "            text = clean_text(response.text)\n",
    "\n",
    "            if text:\n",
    "                f.write(header + \"\\n\")\n",
    "                f.write(text + \"\\n\\n\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y ná»™i dung á»Ÿ {url}\")\n",
    "\n",
    "# Gá»i hÃ m crawl\n",
    "crawl_and_write(urls, \"cmu_about.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027b9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang crawl trang 1...\n",
      "ğŸ” Äang crawl trang 2...\n",
      "ğŸ” Äang crawl trang 3...\n",
      "ğŸ” Äang crawl trang 4...\n",
      "ğŸ” Äang crawl trang 5...\n",
      "ğŸ” Äang crawl trang 6...\n",
      "ğŸ” Äang crawl trang 7...\n",
      "ğŸ” Äang crawl trang 8...\n",
      "ğŸ” Äang crawl trang 9...\n",
      "ğŸ” Äang crawl trang 10...\n",
      "ğŸ” Äang crawl trang 11...\n",
      "ğŸ” Äang crawl trang 12...\n",
      "ğŸ” Äang crawl trang 13...\n",
      "ğŸ” Äang crawl trang 14...\n",
      "ğŸ” Äang crawl trang 15...\n",
      "ğŸ” Äang crawl trang 16...\n",
      "ğŸ” Äang crawl trang 17...\n",
      "ğŸ” Äang crawl trang 18...\n",
      "ğŸ” Äang crawl trang 19...\n",
      "ğŸ” Äang crawl trang 20...\n",
      "ğŸ” Äang crawl trang 21...\n",
      "ğŸ” Äang crawl trang 22...\n",
      "ğŸ” Äang crawl trang 23...\n",
      "ğŸ” Äang crawl trang 24...\n",
      "ğŸ” Äang crawl trang 25...\n",
      "ğŸ” Äang crawl trang 26...\n",
      "ğŸ” Äang crawl trang 27...\n",
      "ğŸ” Äang crawl trang 28...\n",
      "ğŸ” Äang crawl trang 29...\n",
      "ğŸ” Äang crawl trang 30...\n",
      "ğŸ” Äang crawl trang 31...\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o 'Downtown Pittsburgh events calendar.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_all_events(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 32):  # tá»« 1 Ä‘áº¿n 31\n",
    "            url = f\"https://downtownpittsburgh.com/events/?n=5&d={page}&y=2025\"\n",
    "            print(f\"ğŸ” Äang crawl trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ Lá»—i truy cáº­p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            h1_tags = soup.find_all('h1')\n",
    "\n",
    "            if not h1_tags:\n",
    "                print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y sá»± kiá»‡n trÃªn trang {page}\")\n",
    "                continue\n",
    "\n",
    "            for h1 in h1_tags:\n",
    "                link_tag = h1.find('a')\n",
    "                if not link_tag:\n",
    "                    continue\n",
    "                title = link_tag.get_text(strip=True)\n",
    "                href = link_tag.get('href', '')\n",
    "                full_url = 'https://downtownpittsburgh.com' + href\n",
    "\n",
    "                sibling = h1.next_sibling\n",
    "                date_text = None\n",
    "                description_parts = []\n",
    "\n",
    "                while sibling:\n",
    "                    if not hasattr(sibling, 'get_text'):\n",
    "                        sibling = sibling.next_sibling\n",
    "                        continue\n",
    "\n",
    "                    text = sibling.get_text(strip=True)\n",
    "                    if text:\n",
    "                        if not date_text and ('am' in text.lower() or 'pm' in text.lower() or '-' in text):\n",
    "                            date_text = text\n",
    "                        elif text == \"READ MORE\":\n",
    "                            break\n",
    "                        else:\n",
    "                            description_parts.append(text)\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "                if not date_text:\n",
    "                    date_text = \"No date\"\n",
    "                description = \" \".join(description_parts).strip() if description_parts else \"No description\"\n",
    "\n",
    "                f.write(f\"## [{title}]({full_url})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date_text}\\n\")\n",
    "                f.write(f\"**Description**: {description}\\n\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o '{output_file}'\")\n",
    "\n",
    "\n",
    "crawl_all_events(\"Downtown Pittsburgh events calendar.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "477a0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang xá»­ lÃ½ trang 1...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 2...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 3...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 4...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 5...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 6...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 7...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 8...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 9...\n",
      "ğŸ” Äang xá»­ lÃ½ trang 10...\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u sá»± kiá»‡n vÃ o 'citypaper_events.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl_citypaper_detailed(output_file):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for page in range(1, 11):\n",
    "            url = f\"https://www.pghcitypaper.com/pittsburgh/EventSearch?page={page}&v=d\"\n",
    "            print(f\"ğŸ” Äang xá»­ lÃ½ trang {page}...\")\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ Lá»—i truy cáº­p {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            events = soup.find_all('p', class_='fdn-teaser-headline')\n",
    "\n",
    "            for headline in events:\n",
    "                # Láº¥y tiÃªu Ä‘á» vÃ  link\n",
    "                a_tag = headline.find('a', href=True)\n",
    "                if not a_tag:\n",
    "                    continue\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                link = a_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://www.pghcitypaper.com' + link\n",
    "\n",
    "                # TÃ¬m cÃ¡c pháº§n cÃ²n láº¡i thÃ´ng qua cha cá»§a headline\n",
    "                parent = headline.find_parent()\n",
    "                date_tag = parent.find_next('p', class_='fdn-teaser-subheadline')\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"No date\"\n",
    "\n",
    "                loc_tag = parent.find_next('a', class_='fdn-event-teaser-location-link')\n",
    "                location = loc_tag.get_text(strip=True) if loc_tag else \"No location\"\n",
    "\n",
    "                addr_tag = parent.find_next('p', class_='fdn-inline-split-list')\n",
    "                address = addr_tag.get_text(strip=True) if addr_tag else \"No address\"\n",
    "\n",
    "                desc_tag = parent.find_next('div', class_='fdn-teaser-description')\n",
    "                desc = desc_tag.get_text(strip=True) if desc_tag else \"No description\"\n",
    "\n",
    "                # Ghi ra file\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date & Time**: {date}\\n\")\n",
    "                f.write(f\"**Location**: {location}, {address}\\n\")\n",
    "                f.write(f\"**Description**: {desc}\\n\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u sá»± kiá»‡n vÃ o '{output_file}'\")\n",
    "\n",
    "# Gá»i hÃ m:\n",
    "crawl_citypaper_detailed(\"citypaper_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6386f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250401\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250402\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250403\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250404\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250405\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250406\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250407\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250408\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250409\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250410\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250411\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250412\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250413\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250414\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250415\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250416\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250417\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250418\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250419\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250420\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250421\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250422\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250423\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250424\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250425\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250426\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250427\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250428\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250429\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250430\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250501\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250502\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250503\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250504\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250505\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250506\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250507\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250508\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250509\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250510\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250511\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250512\n",
      "ğŸ” Äang xá»­ lÃ½: https://events.cmu.edu/day/date/20250513\n",
      "\n",
      "âœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o 'cmu_events.txt'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def crawl_cmu_events_selenium(start_date, end_date, output_file):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # khÃ´ng má»Ÿ cá»­a sá»• trÃ¬nh duyá»‡t\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime('%Y%m%d')\n",
    "            url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "            print(f\"ğŸ” Äang xá»­ lÃ½: {url}\")\n",
    "\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # Ä‘á»£i trang load JS xong\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            events = soup.find_all('div', class_='lw_cal_event_info')\n",
    "\n",
    "            if not events:\n",
    "                current_date += timedelta(days=1)\n",
    "                continue\n",
    "\n",
    "            for event in events:\n",
    "                title_tag = event.find('div', class_='lw_events_title')\n",
    "                title = title_tag.a.get_text(strip=True) if title_tag and title_tag.a else \"KhÃ´ng cÃ³ tiÃªu Ä‘á»\"\n",
    "                link = title_tag.a['href'] if title_tag and title_tag.a else \"#\"\n",
    "                if not link.startswith('http'):\n",
    "                    link = 'https://events.cmu.edu' + link\n",
    "\n",
    "                location = event.find('div', class_='lw_events_location')\n",
    "                time_tag = event.find('div', class_='lw_events_time')\n",
    "                summary = event.find('div', class_='lw_events_summary')\n",
    "\n",
    "                f.write(f\"## [{title}]({link})\\n\")\n",
    "                f.write(f\"**Date**: {current_date.strftime('%Y-%m-%d')}\\n\")\n",
    "                f.write(f\"**Time**: {time_tag.get_text(strip=True) if time_tag else 'KhÃ´ng rÃµ'}\\n\")\n",
    "                f.write(f\"**Location**: {location.get_text(strip=True) if location else 'KhÃ´ng rÃµ'}\\n\")\n",
    "                f.write(f\"**Summary**: {summary.get_text(strip=True) if summary else 'KhÃ´ng cÃ³'}\\n\\n\")\n",
    "\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\nâœ… ÄÃ£ lÆ°u táº¥t cáº£ sá»± kiá»‡n vÃ o '{output_file}'\")\n",
    "\n",
    "# Gá»i hÃ m\n",
    "start = datetime(2025, 4, 1)\n",
    "end = datetime(2025, 5, 13)\n",
    "crawl_cmu_events_selenium(start, end, \"cmu_events.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a5f43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o pittsburgh_events.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  khoáº£ng tráº¯ng thá»«a\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "def crawl_events():\n",
    "    url = \"https://www.visitpittsburgh.com/events-festivals/?page=11\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Gá»­i yÃªu cáº§u GET Ä‘áº¿n trang web\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # PhÃ¢n tÃ­ch HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # TÃ¬m táº¥t cáº£ cÃ¡c sá»± kiá»‡n\n",
    "        events = soup.find_all('div', class_='card card--common card--listing')\n",
    "        \n",
    "        # Má»Ÿ file Ä‘á»ƒ lÆ°u dá»¯ liá»‡u\n",
    "        with open('pittsburgh_events.txt', 'w', encoding='utf-8') as f:\n",
    "            for event in events:\n",
    "                # Láº¥y tiÃªu Ä‘á»\n",
    "                title_elem = event.find('a', class_='card_heading')\n",
    "                title = clean_text(title_elem.text) if title_elem else 'N/A'\n",
    "                \n",
    "                # Láº¥y ngÃ y\n",
    "                date_elem = event.find('p', class_='date-heading card_date-heading')\n",
    "                date = clean_text(date_elem.text) if date_elem else 'N/A'\n",
    "                \n",
    "                # Láº¥y Ä‘á»‹a chá»‰\n",
    "                address_elem = event.find('div', class_='card____address')\n",
    "                address = clean_text(address_elem.text) if address_elem else 'N/A'\n",
    "                \n",
    "                # Láº¥y sá»‘ Ä‘iá»‡n thoáº¡i\n",
    "                phone_elem = event.find('div', class_='card____phone')\n",
    "                phone = clean_text(phone_elem.text) if phone_elem else 'N/A'\n",
    "                \n",
    "                # Ghi vÃ o file\n",
    "                f.write(f\"Title: {title}\\n\")\n",
    "                f.write(f\"Date: {date}\\n\")\n",
    "                f.write(f\"Address: {address}\\n\")\n",
    "                f.write(f\"Phone: {phone}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                \n",
    "        print(\"Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o pittsburgh_events.txt\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Lá»—i khi truy cáº­p trang web: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lá»—i: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_events()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
